Using TensorFlow backend.
running build_ext
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
state_input (InputLayer)     (None, 1, 10)             0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 10)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 400)               4400      
_________________________________________________________________
activation_1 (Activation)    (None, 400)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 300)               120300    
_________________________________________________________________
activation_2 (Activation)    (None, 300)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 602       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 2)                 0         
=================================================================
Total params: 125,302
Trainable params: 125,302
Non-trainable params: 0
_________________________________________________________________
None
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
observation_input (InputLayer)  (None, 1, 10)        0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       (None, 2)            0                                            
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 10)           0           observation_input[0][0]          
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 12)           0           action_input[0][0]               
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 400)          5200        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 400)          0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 300)          120300      activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 300)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            301         activation_5[0][0]               
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 1)            0           dense_6[0][0]                    
==================================================================================================
Total params: 125,801
Trainable params: 125,801
Non-trainable params: 0
__________________________________________________________________________________________________
None
2019-03-13 12:58:36.426885: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Training for 1600000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 58s 6ms/step - reward: -65.2378
11 episodes - episode_reward: -6263.448 [-6918.729, -5595.423] - loss: 57.600 - mean_squared_error: 115.201 - mean_q: -310.719 - reward_fwd: -65.237 - reward_ctrl: -0.001

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 57s 6ms/step - reward: -57.5950
11 episodes - episode_reward: -5754.097 [-5991.299, -5555.526] - loss: 302.818 - mean_squared_error: 605.635 - mean_q: -798.640 - reward_fwd: -57.594 - reward_ctrl: -0.001

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 58s 6ms/step - reward: -55.6984
11 episodes - episode_reward: -5659.687 [-6446.207, -5420.197] - loss: 803.496 - mean_squared_error: 1606.991 - mean_q: -1229.050 - reward_fwd: -55.697 - reward_ctrl: -0.001

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 60s 6ms/step - reward: -56.1297
11 episodes - episode_reward: -5734.065 [-6200.740, -5605.728] - loss: 1352.551 - mean_squared_error: 2705.102 - mean_q: -1619.932 - reward_fwd: -56.129 - reward_ctrl: -0.001

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 61s 6ms/step - reward: -55.5190
11 episodes - episode_reward: -5664.533 [-5770.721, -5564.100] - loss: 2083.995 - mean_squared_error: 4167.990 - mean_q: -1961.152 - reward_fwd: -55.519 - reward_ctrl: -0.000

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 63s 6ms/step - reward: -52.1791
11 episodes - episode_reward: -5494.796 [-5597.402, -5162.704] - loss: 2560.156 - mean_squared_error: 5120.311 - mean_q: -2251.410 - reward_fwd: -52.179 - reward_ctrl: -0.000

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 65s 7ms/step - reward: -55.9771
11 episodes - episode_reward: -5623.663 [-5743.983, -5519.404] - loss: 3076.234 - mean_squared_error: 6152.467 - mean_q: -2512.942 - reward_fwd: -55.976 - reward_ctrl: -0.001

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 67s 7ms/step - reward: -56.8417
11 episodes - episode_reward: -5663.652 [-5759.551, -5411.523] - loss: 3753.054 - mean_squared_error: 7506.107 - mean_q: -2748.031 - reward_fwd: -56.841 - reward_ctrl: -0.001

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 70s 7ms/step - reward: -50.8024
12 episodes - episode_reward: -5474.401 [-5628.386, -5149.215] - loss: 4278.488 - mean_squared_error: 8556.977 - mean_q: -2957.880 - reward_fwd: -50.802 - reward_ctrl: -0.001

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 73s 7ms/step - reward: -50.5989
11 episodes - episode_reward: -5430.955 [-5637.468, -5041.666] - loss: 4688.251 - mean_squared_error: 9376.502 - mean_q: -3114.394 - reward_fwd: -50.598 - reward_ctrl: -0.001

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 75s 8ms/step - reward: -51.5913
11 episodes - episode_reward: -5527.150 [-5689.477, -5088.325] - loss: 5509.001 - mean_squared_error: 11018.002 - mean_q: -3263.714 - reward_fwd: -51.590 - reward_ctrl: -0.001

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 78s 8ms/step - reward: -44.4281
11 episodes - episode_reward: -5311.716 [-5826.892, -4837.089] - loss: 5625.523 - mean_squared_error: 11251.047 - mean_q: -3359.894 - reward_fwd: -44.426 - reward_ctrl: -0.003

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 80s 8ms/step - reward: -38.1233
11 episodes - episode_reward: -5163.571 [-5588.395, -4844.975] - loss: 6373.874 - mean_squared_error: 12747.748 - mean_q: -3417.144 - reward_fwd: -38.118 - reward_ctrl: -0.005

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: -24.6654
11 episodes - episode_reward: -4760.701 [-5455.091, -4343.537] - loss: 5829.382 - mean_squared_error: 11658.764 - mean_q: -3348.327 - reward_fwd: -24.655 - reward_ctrl: -0.011

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: -28.3195
11 episodes - episode_reward: -4918.491 [-5299.505, -4320.023] - loss: 6172.231 - mean_squared_error: 12344.463 - mean_q: -3234.715 - reward_fwd: -28.310 - reward_ctrl: -0.009

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: -42.3269
11 episodes - episode_reward: -5352.056 [-5972.518, -4529.316] - loss: 5552.387 - mean_squared_error: 11104.773 - mean_q: -3168.725 - reward_fwd: -42.317 - reward_ctrl: -0.009

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 101s 10ms/step - reward: -26.2447
11 episodes - episode_reward: -5151.012 [-5771.156, -4672.473] - loss: 5470.711 - mean_squared_error: 10941.422 - mean_q: -3056.425 - reward_fwd: -26.226 - reward_ctrl: -0.019

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 105s 11ms/step - reward: -33.2386
12 episodes - episode_reward: -5046.335 [-5958.257, -4468.565] - loss: 4998.661 - mean_squared_error: 9997.322 - mean_q: -2923.327 - reward_fwd: -33.223 - reward_ctrl: -0.015

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 108s 11ms/step - reward: -22.9249
11 episodes - episode_reward: -4737.052 [-5479.168, -4375.907] - loss: 4768.107 - mean_squared_error: 9536.215 - mean_q: -2861.204 - reward_fwd: -22.906 - reward_ctrl: -0.018

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 111s 11ms/step - reward: -20.3421
11 episodes - episode_reward: -4563.940 [-5301.876, -4101.005] - loss: 4611.899 - mean_squared_error: 9223.799 - mean_q: -2750.153 - reward_fwd: -20.328 - reward_ctrl: -0.014

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 114s 11ms/step - reward: -18.3798
11 episodes - episode_reward: -4451.662 [-5273.996, -4122.756] - loss: 3671.140 - mean_squared_error: 7342.279 - mean_q: -2680.190 - reward_fwd: -18.366 - reward_ctrl: -0.014

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 115s 11ms/step - reward: -27.2423
11 episodes - episode_reward: -4816.587 [-5547.983, -4429.659] - loss: 4143.204 - mean_squared_error: 8286.408 - mean_q: -2628.543 - reward_fwd: -27.227 - reward_ctrl: -0.015

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 115s 12ms/step - reward: -17.9686
11 episodes - episode_reward: -4609.304 [-5110.168, -4264.070] - loss: 4163.367 - mean_squared_error: 8326.734 - mean_q: -2586.210 - reward_fwd: -17.955 - reward_ctrl: -0.014

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 117s 12ms/step - reward: -15.9480
11 episodes - episode_reward: -4396.933 [-4549.597, -3984.971] - loss: 3966.375 - mean_squared_error: 7932.749 - mean_q: -2513.917 - reward_fwd: -15.933 - reward_ctrl: -0.015

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 119s 12ms/step - reward: -13.6948
11 episodes - episode_reward: -4186.228 [-4460.730, -4040.699] - loss: 3452.835 - mean_squared_error: 6905.670 - mean_q: -2451.951 - reward_fwd: -13.682 - reward_ctrl: -0.013

Interval 26 (250000 steps performed)
10000/10000 [==============================] - 125s 13ms/step - reward: -18.6088
11 episodes - episode_reward: -4328.734 [-4926.648, -3925.296] - loss: 2909.930 - mean_squared_error: 5819.861 - mean_q: -2364.595 - reward_fwd: -18.593 - reward_ctrl: -0.016

Interval 27 (260000 steps performed)
10000/10000 [==============================] - 131s 13ms/step - reward: -38.8132
12 episodes - episode_reward: -5229.111 [-5754.467, -4238.385] - loss: 3321.454 - mean_squared_error: 6642.908 - mean_q: -2323.044 - reward_fwd: -38.793 - reward_ctrl: -0.020

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 133s 13ms/step - reward: -20.2656
11 episodes - episode_reward: -4471.813 [-5609.151, -3952.247] - loss: 3399.176 - mean_squared_error: 6798.351 - mean_q: -2312.216 - reward_fwd: -20.253 - reward_ctrl: -0.013

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 137s 14ms/step - reward: -18.5248
11 episodes - episode_reward: -4117.191 [-4891.728, -3896.229] - loss: 3315.507 - mean_squared_error: 6631.014 - mean_q: -2236.155 - reward_fwd: -18.510 - reward_ctrl: -0.014

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 132s 13ms/step - reward: -15.7138
11 episodes - episode_reward: -4233.047 [-4765.409, -3881.985] - loss: 3115.920 - mean_squared_error: 6231.839 - mean_q: -2170.560 - reward_fwd: -15.700 - reward_ctrl: -0.014

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 135s 14ms/step - reward: -13.5890
11 episodes - episode_reward: -4105.193 [-4315.280, -3788.808] - loss: 3041.422 - mean_squared_error: 6082.844 - mean_q: -2107.927 - reward_fwd: -13.574 - reward_ctrl: -0.015

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 138s 14ms/step - reward: -12.8109
11 episodes - episode_reward: -4102.855 [-4759.971, -3730.996] - loss: 2643.162 - mean_squared_error: 5286.323 - mean_q: -2042.143 - reward_fwd: -12.793 - reward_ctrl: -0.017

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 141s 14ms/step - reward: -12.6357
11 episodes - episode_reward: -3950.299 [-4118.996, -3724.339] - loss: 2825.264 - mean_squared_error: 5650.528 - mean_q: -1986.004 - reward_fwd: -12.619 - reward_ctrl: -0.017

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 144s 14ms/step - reward: -11.7807
11 episodes - episode_reward: -3972.678 [-4401.093, -3751.193] - loss: 2577.106 - mean_squared_error: 5154.211 - mean_q: -1925.783 - reward_fwd: -11.765 - reward_ctrl: -0.016

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 152s 15ms/step - reward: -11.1466
11 episodes - episode_reward: -3936.047 [-4178.667, -3670.546] - loss: 2289.707 - mean_squared_error: 4579.414 - mean_q: -1862.099 - reward_fwd: -11.129 - reward_ctrl: -0.017

Interval 36 (350000 steps performed)
10000/10000 [==============================] - 156s 16ms/step - reward: -12.4546
12 episodes - episode_reward: -3967.143 [-4328.005, -3738.509] - loss: 2142.937 - mean_squared_error: 4285.875 - mean_q: -1816.034 - reward_fwd: -12.434 - reward_ctrl: -0.020

Interval 37 (360000 steps performed)
10000/10000 [==============================] - 159s 16ms/step - reward: -11.9112
11 episodes - episode_reward: -3959.752 [-4220.831, -3728.851] - loss: 2453.078 - mean_squared_error: 4906.156 - mean_q: -1769.495 - reward_fwd: -11.891 - reward_ctrl: -0.021

Interval 38 (370000 steps performed)
10000/10000 [==============================] - 162s 16ms/step - reward: -10.5978
11 episodes - episode_reward: -3827.262 [-3997.626, -3618.310] - loss: 1845.037 - mean_squared_error: 3690.074 - mean_q: -1714.609 - reward_fwd: -10.576 - reward_ctrl: -0.022

Interval 39 (380000 steps performed)
10000/10000 [==============================] - 165s 16ms/step - reward: -10.3367
11 episodes - episode_reward: -3879.207 [-4232.265, -3579.435] - loss: 1907.310 - mean_squared_error: 3814.620 - mean_q: -1668.804 - reward_fwd: -10.316 - reward_ctrl: -0.021

Interval 40 (390000 steps performed)
10000/10000 [==============================] - 173s 17ms/step - reward: -10.1573
11 episodes - episode_reward: -3836.892 [-3996.136, -3688.469] - loss: 1711.348 - mean_squared_error: 3422.697 - mean_q: -1637.749 - reward_fwd: -10.137 - reward_ctrl: -0.020

Interval 41 (400000 steps performed)
10000/10000 [==============================] - 166s 17ms/step - reward: -10.1845
11 episodes - episode_reward: -3766.930 [-3890.943, -3625.965] - loss: 2156.032 - mean_squared_error: 4312.065 - mean_q: -1603.403 - reward_fwd: -10.164 - reward_ctrl: -0.021

Interval 42 (410000 steps performed)
10000/10000 [==============================] - 169s 17ms/step - reward: -11.2178
11 episodes - episode_reward: -3962.554 [-4241.100, -3702.070] - loss: 1851.250 - mean_squared_error: 3702.500 - mean_q: -1560.145 - reward_fwd: -11.192 - reward_ctrl: -0.025

Interval 43 (420000 steps performed)
10000/10000 [==============================] - 172s 17ms/step - reward: -10.8022
11 episodes - episode_reward: -3938.618 [-4146.411, -3712.900] - loss: 1887.189 - mean_squared_error: 3774.377 - mean_q: -1529.380 - reward_fwd: -10.776 - reward_ctrl: -0.027

Interval 44 (430000 steps performed)
10000/10000 [==============================] - 178s 18ms/step - reward: -9.5819
11 episodes - episode_reward: -3794.331 [-3996.126, -3622.827] - loss: 1960.487 - mean_squared_error: 3920.974 - mean_q: -1496.899 - reward_fwd: -9.559 - reward_ctrl: -0.023

Interval 45 (440000 steps performed)
10000/10000 [==============================] - 184s 18ms/step - reward: -9.5461
12 episodes - episode_reward: -3789.682 [-3959.648, -3632.266] - loss: 1392.546 - mean_squared_error: 2785.091 - mean_q: -1472.487 - reward_fwd: -9.524 - reward_ctrl: -0.023

Interval 46 (450000 steps performed)
10000/10000 [==============================] - 187s 19ms/step - reward: -9.5647
11 episodes - episode_reward: -3735.910 [-3879.637, -3581.201] - loss: 1416.459 - mean_squared_error: 2832.918 - mean_q: -1452.965 - reward_fwd: -9.543 - reward_ctrl: -0.022

Interval 47 (460000 steps performed)
10000/10000 [==============================] - 189s 19ms/step - reward: -9.4208
11 episodes - episode_reward: -3703.755 [-3917.581, -3579.030] - loss: 1593.044 - mean_squared_error: 3186.087 - mean_q: -1436.549 - reward_fwd: -9.397 - reward_ctrl: -0.024

Interval 48 (470000 steps performed)
10000/10000 [==============================] - 192s 19ms/step - reward: -9.9791
11 episodes - episode_reward: -3765.799 [-3957.322, -3603.326] - loss: 1672.688 - mean_squared_error: 3345.375 - mean_q: -1408.290 - reward_fwd: -9.955 - reward_ctrl: -0.024

Interval 49 (480000 steps performed)
10000/10000 [==============================] - 200s 20ms/step - reward: -10.8673
11 episodes - episode_reward: -3872.964 [-4552.968, -3591.321] - loss: 1620.078 - mean_squared_error: 3240.157 - mean_q: -1383.585 - reward_fwd: -10.842 - reward_ctrl: -0.026

Interval 50 (490000 steps performed)
10000/10000 [==============================] - 194s 19ms/step - reward: -9.3415
11 episodes - episode_reward: -3700.919 [-3889.165, -3584.461] - loss: 1462.527 - mean_squared_error: 2925.054 - mean_q: -1362.925 - reward_fwd: -9.320 - reward_ctrl: -0.022

Interval 51 (500000 steps performed)
10000/10000 [==============================] - 198s 20ms/step - reward: -9.4633
11 episodes - episode_reward: -3698.813 [-3864.713, -3531.048] - loss: 1347.021 - mean_squared_error: 2694.042 - mean_q: -1334.246 - reward_fwd: -9.435 - reward_ctrl: -0.028

Interval 52 (510000 steps performed)
10000/10000 [==============================] - 200s 20ms/step - reward: -9.9585
11 episodes - episode_reward: -3626.238 [-3777.712, -3527.874] - loss: 1653.310 - mean_squared_error: 3306.620 - mean_q: -1311.838 - reward_fwd: -9.934 - reward_ctrl: -0.025

Interval 53 (520000 steps performed)
10000/10000 [==============================] - 203s 20ms/step - reward: -11.2256
11 episodes - episode_reward: -3778.638 [-4147.537, -3565.731] - loss: 1288.935 - mean_squared_error: 2577.870 - mean_q: -1285.943 - reward_fwd: -11.196 - reward_ctrl: -0.030

Interval 54 (530000 steps performed)
10000/10000 [==============================] - 206s 21ms/step - reward: -10.1786
12 episodes - episode_reward: -3687.096 [-3878.150, -3488.827] - loss: 1336.540 - mean_squared_error: 2673.081 - mean_q: -1271.095 - reward_fwd: -10.154 - reward_ctrl: -0.025

Interval 55 (540000 steps performed)
10000/10000 [==============================] - 214s 21ms/step - reward: -11.3347
11 episodes - episode_reward: -3760.572 [-4045.684, -3533.332] - loss: 1200.371 - mean_squared_error: 2400.742 - mean_q: -1249.206 - reward_fwd: -11.306 - reward_ctrl: -0.029

Interval 56 (550000 steps performed)
10000/10000 [==============================] - 220s 22ms/step - reward: -10.2274
11 episodes - episode_reward: -3781.207 [-4009.549, -3571.195] - loss: 1214.097 - mean_squared_error: 2428.194 - mean_q: -1235.259 - reward_fwd: -10.200 - reward_ctrl: -0.027

Interval 57 (560000 steps performed)
 9984/10000 [============================>.] - ETA: 0s - reward: -9.7161^Cdone, took 7617.314 seconds
[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
   0.00000000e+00  0.00000000e+00]
 [ 5.43620068e-03  0.00000000e+00  1.17787972e-01 ...  1.17787972e+01
  -2.44639903e+01  2.44639903e+01]
 [ 9.61519830e-03  1.97448301e-03  2.19495344e-01 ...  1.01707372e+01
  -2.20366699e+01  2.19299845e+01]
 ...
 [ 6.06966634e-01 -5.08326551e-01  6.52778477e-01 ...  6.45661716e-03
  -1.33044029e-03  9.70679208e-03]
 [ 6.06992202e-01 -5.08333539e-01  6.52842694e-01 ...  6.42171331e-03
  -1.31951271e-03  9.71814903e-03]
 [ 6.07017723e-01 -5.08340526e-01  6.52906629e-01 ...  6.39351943e-03
  -1.31260057e-03  9.71530610e-03]]

