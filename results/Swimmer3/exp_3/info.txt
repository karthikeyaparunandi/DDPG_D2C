Using TensorFlow backend.
running build_ext
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
state_input (InputLayer)     (None, 1, 8)              0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 8)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 400)               3600      
_________________________________________________________________
activation_1 (Activation)    (None, 400)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 300)               120300    
_________________________________________________________________
activation_2 (Activation)    (None, 300)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 602       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 2)                 0         
=================================================================
Total params: 124,502
Trainable params: 124,502
Non-trainable params: 0
_________________________________________________________________
None
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
observation_input (InputLayer)  (None, 1, 8)         0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       (None, 2)            0                                            
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 8)            0           observation_input[0][0]          
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 10)           0           action_input[0][0]               
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 400)          4400        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 400)          0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 300)          120300      activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 300)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            301         activation_5[0][0]               
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 1)            0           dense_6[0][0]                    
==================================================================================================
Total params: 125,001
Trainable params: 125,001
Non-trainable params: 0
__________________________________________________________________________________________________
None
2019-02-07 12:28:38.310305: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Training for 1000000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 73s 7ms/step - reward: -62.4397
12 episodes - episode_reward: -6019.656 [-6612.429, -5609.243] - loss: 51.915 - mean_squared_error: 103.831 - mean_q: -292.676 - reward_fwd: -62.439 - reward_ctrl: -0.001

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 78s 8ms/step - reward: -51.6363
13 episodes - episode_reward: -5475.005 [-5717.030, -5310.719] - loss: 330.931 - mean_squared_error: 661.862 - mean_q: -770.957 - reward_fwd: -51.635 - reward_ctrl: -0.001

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 83s 8ms/step - reward: -59.6745
12 episodes - episode_reward: -5787.887 [-6210.602, -5447.424] - loss: 769.445 - mean_squared_error: 1538.889 - mean_q: -1155.474 - reward_fwd: -59.673 - reward_ctrl: -0.002

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: -55.0308
13 episodes - episode_reward: -5652.958 [-6268.467, -5179.130] - loss: 1392.967 - mean_squared_error: 2785.934 - mean_q: -1551.107 - reward_fwd: -55.029 - reward_ctrl: -0.002

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: -62.2204
12 episodes - episode_reward: -5805.314 [-6392.210, -5380.548] - loss: 2110.934 - mean_squared_error: 4221.868 - mean_q: -1855.839 - reward_fwd: -62.219 - reward_ctrl: -0.001

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: -62.0185
13 episodes - episode_reward: -5914.694 [-6439.357, -5757.010] - loss: 3045.974 - mean_squared_error: 6091.949 - mean_q: -2187.277 - reward_fwd: -62.018 - reward_ctrl: -0.001

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: -54.5846
12 episodes - episode_reward: -5724.798 [-5954.809, -5194.905] - loss: 3566.952 - mean_squared_error: 7133.904 - mean_q: -2466.268 - reward_fwd: -54.584 - reward_ctrl: -0.000

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: -46.7191
13 episodes - episode_reward: -5538.632 [-5845.729, -5027.257] - loss: 4278.382 - mean_squared_error: 8556.764 - mean_q: -2693.868 - reward_fwd: -46.718 - reward_ctrl: -0.001

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: -49.4650
12 episodes - episode_reward: -5358.614 [-5666.847, -5081.297] - loss: 5415.121 - mean_squared_error: 10830.241 - mean_q: -2883.487 - reward_fwd: -49.464 - reward_ctrl: -0.001

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 92s 9ms/step - reward: -43.6447
13 episodes - episode_reward: -5159.020 [-5459.750, -4891.135] - loss: 5856.860 - mean_squared_error: 11713.721 - mean_q: -3031.494 - reward_fwd: -43.643 - reward_ctrl: -0.002

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: -45.5932
12 episodes - episode_reward: -5294.187 [-5431.669, -5199.795] - loss: 6272.276 - mean_squared_error: 12544.553 - mean_q: -3106.433 - reward_fwd: -45.593 - reward_ctrl: -0.000

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 96s 10ms/step - reward: -31.6470
13 episodes - episode_reward: -5009.248 [-5329.865, -4802.324] - loss: 6168.289 - mean_squared_error: 12336.578 - mean_q: -3158.573 - reward_fwd: -31.643 - reward_ctrl: -0.004

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 98s 10ms/step - reward: -32.4579
12 episodes - episode_reward: -5044.307 [-5253.988, -4741.479] - loss: 5496.733 - mean_squared_error: 10993.467 - mean_q: -3147.943 - reward_fwd: -32.454 - reward_ctrl: -0.004

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 102s 10ms/step - reward: -24.0386
13 episodes - episode_reward: -4692.828 [-4893.804, -4579.199] - loss: 5496.422 - mean_squared_error: 10992.844 - mean_q: -3137.560 - reward_fwd: -24.033 - reward_ctrl: -0.006

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 104s 10ms/step - reward: -29.2798
12 episodes - episode_reward: -4671.198 [-4834.164, -4585.999] - loss: 5974.585 - mean_squared_error: 11949.170 - mean_q: -3051.828 - reward_fwd: -29.272 - reward_ctrl: -0.008

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 109s 11ms/step - reward: -29.9102
13 episodes - episode_reward: -4841.471 [-5084.386, -4692.073] - loss: 5713.866 - mean_squared_error: 11427.732 - mean_q: -2934.902 - reward_fwd: -29.901 - reward_ctrl: -0.009

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 113s 11ms/step - reward: -27.5397
12 episodes - episode_reward: -5016.880 [-5347.174, -4814.344] - loss: 5103.695 - mean_squared_error: 10207.391 - mean_q: -2884.428 - reward_fwd: -27.532 - reward_ctrl: -0.007

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 116s 12ms/step - reward: -37.0191
13 episodes - episode_reward: -5236.028 [-5831.423, -4896.464] - loss: 5089.792 - mean_squared_error: 10179.583 - mean_q: -2870.218 - reward_fwd: -37.014 - reward_ctrl: -0.005

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 119s 12ms/step - reward: -43.0102
12 episodes - episode_reward: -5262.208 [-5486.689, -4878.340] - loss: 4916.369 - mean_squared_error: 9832.738 - mean_q: -2845.694 - reward_fwd: -43.006 - reward_ctrl: -0.004

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 120s 12ms/step - reward: -30.3580
13 episodes - episode_reward: -5083.775 [-5348.881, -4874.204] - loss: 4792.083 - mean_squared_error: 9584.165 - mean_q: -2857.998 - reward_fwd: -30.348 - reward_ctrl: -0.010

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 121s 12ms/step - reward: -29.3902
12 episodes - episode_reward: -5005.061 [-5935.536, -4504.228] - loss: 4653.155 - mean_squared_error: 9306.310 - mean_q: -2820.537 - reward_fwd: -29.378 - reward_ctrl: -0.013

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 120s 12ms/step - reward: -24.5908
13 episodes - episode_reward: -4680.724 [-4976.111, -4393.704] - loss: 4869.373 - mean_squared_error: 9738.745 - mean_q: -2695.786 - reward_fwd: -24.580 - reward_ctrl: -0.010

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 120s 12ms/step - reward: -34.8848
12 episodes - episode_reward: -4826.934 [-5037.646, -4572.595] - loss: 4153.709 - mean_squared_error: 8307.418 - mean_q: -2525.824 - reward_fwd: -34.872 - reward_ctrl: -0.013

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 122s 12ms/step - reward: -32.2470
13 episodes - episode_reward: -5025.959 [-6181.051, -4590.641] - loss: 3958.531 - mean_squared_error: 7917.061 - mean_q: -2475.076 - reward_fwd: -32.235 - reward_ctrl: -0.012

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 126s 13ms/step - reward: -26.4028
12 episodes - episode_reward: -4696.518 [-4966.301, -4202.185] - loss: 3489.695 - mean_squared_error: 6979.390 - mean_q: -2418.367 - reward_fwd: -26.391 - reward_ctrl: -0.012

Interval 26 (250000 steps performed)
10000/10000 [==============================] - 128s 13ms/step - reward: -23.6252
13 episodes - episode_reward: -4614.860 [-5088.688, -4201.243] - loss: 3693.636 - mean_squared_error: 7387.272 - mean_q: -2360.746 - reward_fwd: -23.616 - reward_ctrl: -0.009

Interval 27 (260000 steps performed)
10000/10000 [==============================] - 130s 13ms/step - reward: -22.2820
12 episodes - episode_reward: -4507.083 [-4708.036, -4355.649] - loss: 3883.016 - mean_squared_error: 7766.032 - mean_q: -2286.396 - reward_fwd: -22.267 - reward_ctrl: -0.015

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 128s 13ms/step - reward: -22.3526
13 episodes - episode_reward: -4595.401 [-4952.527, -4338.426] - loss: 3297.660 - mean_squared_error: 6595.320 - mean_q: -2267.813 - reward_fwd: -22.338 - reward_ctrl: -0.015

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 132s 13ms/step - reward: -25.9891
12 episodes - episode_reward: -4687.867 [-5221.039, -4248.744] - loss: 3295.641 - mean_squared_error: 6591.282 - mean_q: -2235.325 - reward_fwd: -25.977 - reward_ctrl: -0.012

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 145s 15ms/step - reward: -26.6450
13 episodes - episode_reward: -4625.250 [-5036.526, -4418.753] - loss: 3129.778 - mean_squared_error: 6259.556 - mean_q: -2197.802 - reward_fwd: -26.633 - reward_ctrl: -0.012

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 144s 14ms/step - reward: -22.8839
12 episodes - episode_reward: -4398.940 [-4670.605, -4245.324] - loss: 3009.227 - mean_squared_error: 6018.454 - mean_q: -2163.561 - reward_fwd: -22.872 - reward_ctrl: -0.012

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 141s 14ms/step - reward: -22.7547
13 episodes - episode_reward: -4507.126 [-5241.436, -4145.540] - loss: 2548.898 - mean_squared_error: 5097.795 - mean_q: -2141.208 - reward_fwd: -22.740 - reward_ctrl: -0.015

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 143s 14ms/step - reward: -26.5739
12 episodes - episode_reward: -4647.797 [-5710.251, -4079.041] - loss: 2792.776 - mean_squared_error: 5585.553 - mean_q: -2137.293 - reward_fwd: -26.560 - reward_ctrl: -0.014

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 149s 15ms/step - reward: -19.6520
13 episodes - episode_reward: -4439.302 [-4599.171, -4243.213] - loss: 3089.640 - mean_squared_error: 6179.280 - mean_q: -2115.719 - reward_fwd: -19.640 - reward_ctrl: -0.012

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 154s 15ms/step - reward: -21.7523
12 episodes - episode_reward: -4290.447 [-4557.722, -4049.151] - loss: 2850.228 - mean_squared_error: 5700.456 - mean_q: -2067.191 - reward_fwd: -21.741 - reward_ctrl: -0.011

Interval 36 (350000 steps performed)
10000/10000 [==============================] - 156s 16ms/step - reward: -23.9229
13 episodes - episode_reward: -4343.330 [-4712.937, -3915.243] - loss: 2646.752 - mean_squared_error: 5293.504 - mean_q: -2018.648 - reward_fwd: -23.913 - reward_ctrl: -0.010

Interval 37 (360000 steps performed)
10000/10000 [==============================] - 158s 16ms/step - reward: -17.9916
12 episodes - episode_reward: -4145.366 [-4305.355, -3912.550] - loss: 3123.668 - mean_squared_error: 6247.335 - mean_q: -1985.840 - reward_fwd: -17.980 - reward_ctrl: -0.011

Interval 38 (370000 steps performed)
10000/10000 [==============================] - 168s 17ms/step - reward: -18.7096
13 episodes - episode_reward: -4236.578 [-4683.938, -3988.680] - loss: 2693.985 - mean_squared_error: 5387.969 - mean_q: -1961.522 - reward_fwd: -18.698 - reward_ctrl: -0.012

Interval 39 (380000 steps performed)
10000/10000 [==============================] - 168s 17ms/step - reward: -18.3330
12 episodes - episode_reward: -4198.227 [-4498.165, -3961.157] - loss: 2627.641 - mean_squared_error: 5255.282 - mean_q: -1932.647 - reward_fwd: -18.319 - reward_ctrl: -0.014

Interval 40 (390000 steps performed)
10000/10000 [==============================] - 166s 17ms/step - reward: -19.4322
13 episodes - episode_reward: -4259.124 [-4732.878, -3895.406] - loss: 2225.442 - mean_squared_error: 4450.885 - mean_q: -1901.665 - reward_fwd: -19.420 - reward_ctrl: -0.012

Interval 41 (400000 steps performed)
10000/10000 [==============================] - 165s 17ms/step - reward: -18.1758
12 episodes - episode_reward: -4233.461 [-4707.533, -3910.156] - loss: 2398.526 - mean_squared_error: 4797.052 - mean_q: -1876.402 - reward_fwd: -18.163 - reward_ctrl: -0.013

Interval 42 (410000 steps performed)
10000/10000 [==============================] - 173s 17ms/step - reward: -15.6269
13 episodes - episode_reward: -4148.747 [-4362.286, -3977.452] - loss: 2447.719 - mean_squared_error: 4895.437 - mean_q: -1845.364 - reward_fwd: -15.613 - reward_ctrl: -0.014

Interval 43 (420000 steps performed)
10000/10000 [==============================] - 183s 18ms/step - reward: -18.0192
12 episodes - episode_reward: -4253.049 [-4481.735, -4055.510] - loss: 2164.502 - mean_squared_error: 4329.004 - mean_q: -1811.621 - reward_fwd: -18.005 - reward_ctrl: -0.014

Interval 44 (430000 steps performed)
10000/10000 [==============================] - 174s 17ms/step - reward: -17.6794
13 episodes - episode_reward: -4251.001 [-4449.478, -4002.803] - loss: 2301.706 - mean_squared_error: 4603.412 - mean_q: -1779.058 - reward_fwd: -17.664 - reward_ctrl: -0.016

Interval 45 (440000 steps performed)
10000/10000 [==============================] - 170s 17ms/step - reward: -23.5235
12 episodes - episode_reward: -4406.862 [-5099.750, -4088.962] - loss: 1951.869 - mean_squared_error: 3903.737 - mean_q: -1744.312 - reward_fwd: -23.509 - reward_ctrl: -0.014

Interval 46 (450000 steps performed)
10000/10000 [==============================] - 203s 20ms/step - reward: -19.1020
13 episodes - episode_reward: -4304.332 [-4551.814, -4081.473] - loss: 2135.886 - mean_squared_error: 4271.773 - mean_q: -1704.668 - reward_fwd: -19.085 - reward_ctrl: -0.017

Interval 47 (460000 steps performed)
10000/10000 [==============================] - 195s 20ms/step - reward: -18.8549
12 episodes - episode_reward: -4338.886 [-4559.797, -4128.071] - loss: 1850.318 - mean_squared_error: 3700.635 - mean_q: -1685.655 - reward_fwd: -18.833 - reward_ctrl: -0.022

Interval 48 (470000 steps performed)
10000/10000 [==============================] - 193s 19ms/step - reward: -19.8672
13 episodes - episode_reward: -4389.161 [-4900.681, -4028.298] - loss: 2190.292 - mean_squared_error: 4380.585 - mean_q: -1671.436 - reward_fwd: -19.849 - reward_ctrl: -0.018

Interval 49 (480000 steps performed)
10000/10000 [==============================] - 181s 18ms/step - reward: -19.4708
12 episodes - episode_reward: -4290.212 [-4584.871, -4044.521] - loss: 2350.221 - mean_squared_error: 4700.441 - mean_q: -1650.382 - reward_fwd: -19.451 - reward_ctrl: -0.020

Interval 50 (490000 steps performed)
10000/10000 [==============================] - 191s 19ms/step - reward: -28.6441
13 episodes - episode_reward: -4657.460 [-5319.707, -4394.321] - loss: 1780.995 - mean_squared_error: 3561.991 - mean_q: -1624.522 - reward_fwd: -28.630 - reward_ctrl: -0.014

Interval 51 (500000 steps performed)
10000/10000 [==============================] - 201s 20ms/step - reward: -25.7034
12 episodes - episode_reward: -4520.003 [-5130.785, -4150.532] - loss: 2214.495 - mean_squared_error: 4428.991 - mean_q: -1615.515 - reward_fwd: -25.689 - reward_ctrl: -0.014

Interval 52 (510000 steps performed)
10000/10000 [==============================] - 190s 19ms/step - reward: -23.3936
13 episodes - episode_reward: -4621.639 [-5174.336, -4154.830] - loss: 2250.730 - mean_squared_error: 4501.459 - mean_q: -1622.021 - reward_fwd: -23.376 - reward_ctrl: -0.018

Interval 53 (520000 steps performed)
10000/10000 [==============================] - 201s 20ms/step - reward: -16.6695
12 episodes - episode_reward: -4207.612 [-4343.410, -4018.326] - loss: 1748.378 - mean_squared_error: 3496.756 - mean_q: -1625.192 - reward_fwd: -16.652 - reward_ctrl: -0.017

Interval 54 (530000 steps performed)
10000/10000 [==============================] - 207s 21ms/step - reward: -15.9944
13 episodes - episode_reward: -4222.997 [-4361.130, -4012.448] - loss: 2057.931 - mean_squared_error: 4115.862 - mean_q: -1616.405 - reward_fwd: -15.976 - reward_ctrl: -0.018

Interval 55 (540000 steps performed)
10000/10000 [==============================] - 202s 20ms/step - reward: -16.2863
12 episodes - episode_reward: -4214.612 [-4484.629, -4054.434] - loss: 1973.920 - mean_squared_error: 3947.839 - mean_q: -1605.840 - reward_fwd: -16.265 - reward_ctrl: -0.021

Interval 56 (550000 steps performed)
10000/10000 [==============================] - 201s 20ms/step - reward: -14.1625
13 episodes - episode_reward: -4147.275 [-4403.897, -4005.761] - loss: 2218.664 - mean_squared_error: 4437.328 - mean_q: -1599.799 - reward_fwd: -14.143 - reward_ctrl: -0.019

Interval 57 (560000 steps performed)
10000/10000 [==============================] - 198s 20ms/step - reward: -15.9202
12 episodes - episode_reward: -4132.982 [-4356.686, -3966.933] - loss: 1904.930 - mean_squared_error: 3809.860 - mean_q: -1594.631 - reward_fwd: -15.901 - reward_ctrl: -0.020

Interval 58 (570000 steps performed)
10000/10000 [==============================] - 209s 21ms/step - reward: -19.0038
13 episodes - episode_reward: -4341.499 [-4949.235, -4050.614] - loss: 1841.237 - mean_squared_error: 3682.475 - mean_q: -1582.093 - reward_fwd: -18.981 - reward_ctrl: -0.022

Interval 59 (580000 steps performed)
10000/10000 [==============================] - 215s 21ms/step - reward: -16.0916
12 episodes - episode_reward: -4226.535 [-4397.161, -3967.358] - loss: 1684.423 - mean_squared_error: 3368.846 - mean_q: -1574.143 - reward_fwd: -16.071 - reward_ctrl: -0.021

Interval 60 (590000 steps performed)
10000/10000 [==============================] - 212s 21ms/step - reward: -14.9391
13 episodes - episode_reward: -4124.536 [-4344.020, -3949.108] - loss: 1593.606 - mean_squared_error: 3187.213 - mean_q: -1563.585 - reward_fwd: -14.918 - reward_ctrl: -0.021

Interval 61 (600000 steps performed)
10000/10000 [==============================] - 215s 21ms/step - reward: -15.1083
12 episodes - episode_reward: -4044.882 [-4256.045, -3900.664] - loss: 1635.247 - mean_squared_error: 3270.494 - mean_q: -1549.492 - reward_fwd: -15.089 - reward_ctrl: -0.019

Interval 62 (610000 steps performed)
10000/10000 [==============================] - 205s 21ms/step - reward: -14.6847
13 episodes - episode_reward: -4139.635 [-4538.492, -4009.757] - loss: 1754.569 - mean_squared_error: 3509.138 - mean_q: -1534.624 - reward_fwd: -14.668 - reward_ctrl: -0.017

Interval 63 (620000 steps performed)
10000/10000 [==============================] - 205s 21ms/step - reward: -15.6649
12 episodes - episode_reward: -4188.073 [-4541.751, -3994.689] - loss: 1576.279 - mean_squared_error: 3152.558 - mean_q: -1515.515 - reward_fwd: -15.644 - reward_ctrl: -0.021

Interval 64 (630000 steps performed)
10000/10000 [==============================] - 218s 22ms/step - reward: -15.5015
13 episodes - episode_reward: -4220.244 [-4571.831, -4023.018] - loss: 1648.121 - mean_squared_error: 3296.242 - mean_q: -1506.421 - reward_fwd: -15.483 - reward_ctrl: -0.019

Interval 65 (640000 steps performed)
10000/10000 [==============================] - 222s 22ms/step - reward: -15.6501
12 episodes - episode_reward: -4150.479 [-4378.433, -4005.363] - loss: 1892.461 - mean_squared_error: 3784.923 - mean_q: -1498.882 - reward_fwd: -15.630 - reward_ctrl: -0.020

Interval 66 (650000 steps performed)
10000/10000 [==============================] - 224s 22ms/step - reward: -15.0451
13 episodes - episode_reward: -4141.105 [-4299.159, -3975.939] - loss: 1643.963 - mean_squared_error: 3287.927 - mean_q: -1492.090 - reward_fwd: -15.027 - reward_ctrl: -0.018

Interval 67 (660000 steps performed)
10000/10000 [==============================] - 222s 22ms/step - reward: -15.7769
12 episodes - episode_reward: -4130.567 [-4265.862, -3948.758] - loss: 1612.721 - mean_squared_error: 3225.442 - mean_q: -1473.960 - reward_fwd: -15.758 - reward_ctrl: -0.018

Interval 68 (670000 steps performed)
10000/10000 [==============================] - 227s 23ms/step - reward: -15.6338
13 episodes - episode_reward: -4251.214 [-4646.756, -3999.286] - loss: 1549.139 - mean_squared_error: 3098.278 - mean_q: -1449.181 - reward_fwd: -15.614 - reward_ctrl: -0.019

Interval 69 (680000 steps performed)
10000/10000 [==============================] - 220s 22ms/step - reward: -16.7487
12 episodes - episode_reward: -4345.553 [-4681.596, -4123.272] - loss: 1672.192 - mean_squared_error: 3344.385 - mean_q: -1432.328 - reward_fwd: -16.730 - reward_ctrl: -0.019

Interval 70 (690000 steps performed)
10000/10000 [==============================] - 219s 22ms/step - reward: -15.8208
13 episodes - episode_reward: -4241.650 [-4698.510, -4021.629] - loss: 1465.704 - mean_squared_error: 2931.407 - mean_q: -1420.806 - reward_fwd: -15.803 - reward_ctrl: -0.018

Interval 71 (700000 steps performed)
10000/10000 [==============================] - 235s 23ms/step - reward: -17.9647
12 episodes - episode_reward: -4327.078 [-4692.531, -3994.060] - loss: 1421.143 - mean_squared_error: 2842.285 - mean_q: -1408.655 - reward_fwd: -17.946 - reward_ctrl: -0.019

Interval 72 (710000 steps performed)
10000/10000 [==============================] - 234s 23ms/step - reward: -17.0603
13 episodes - episode_reward: -4336.641 [-4925.042, -4024.147] - loss: 1470.985 - mean_squared_error: 2941.969 - mean_q: -1398.901 - reward_fwd: -17.039 - reward_ctrl: -0.021

Interval 73 (720000 steps performed)
10000/10000 [==============================] - 234s 23ms/step - reward: -15.6681
12 episodes - episode_reward: -4124.576 [-4464.231, -3874.035] - loss: 1185.838 - mean_squared_error: 2371.676 - mean_q: -1398.549 - reward_fwd: -15.648 - reward_ctrl: -0.020

Interval 74 (730000 steps performed)
10000/10000 [==============================] - 237s 24ms/step - reward: -15.9001
13 episodes - episode_reward: -4242.172 [-4717.819, -3973.769] - loss: 1217.374 - mean_squared_error: 2434.748 - mean_q: -1395.807 - reward_fwd: -15.880 - reward_ctrl: -0.021

Interval 75 (740000 steps performed)
10000/10000 [==============================] - 257s 26ms/step - reward: -16.1512
12 episodes - episode_reward: -4143.344 [-4296.343, -3903.996] - loss: 1268.085 - mean_squared_error: 2536.170 - mean_q: -1392.189 - reward_fwd: -16.130 - reward_ctrl: -0.021

Interval 76 (750000 steps performed)
10000/10000 [==============================] - 250s 25ms/step - reward: -17.4895
13 episodes - episode_reward: -4150.986 [-4346.830, -3985.031] - loss: 1509.664 - mean_squared_error: 3019.328 - mean_q: -1383.858 - reward_fwd: -17.470 - reward_ctrl: -0.020

Interval 77 (760000 steps performed)
10000/10000 [==============================] - 255s 25ms/step - reward: -16.3568
12 episodes - episode_reward: -4142.394 [-4293.461, -4021.146] - loss: 1421.531 - mean_squared_error: 2843.062 - mean_q: -1376.855 - reward_fwd: -16.333 - reward_ctrl: -0.024

Interval 78 (770000 steps performed)
10000/10000 [==============================] - 260s 26ms/step - reward: -19.9435
13 episodes - episode_reward: -4228.385 [-4456.817, -4102.191] - loss: 1343.047 - mean_squared_error: 2686.094 - mean_q: -1371.651 - reward_fwd: -19.919 - reward_ctrl: -0.024

Interval 79 (780000 steps performed)
10000/10000 [==============================] - 252s 25ms/step - reward: -17.0948
12 episodes - episode_reward: -4171.983 [-4399.922, -3994.601] - loss: 1391.033 - mean_squared_error: 2782.067 - mean_q: -1368.864 - reward_fwd: -17.073 - reward_ctrl: -0.021

Interval 80 (790000 steps performed)
10000/10000 [==============================] - 238s 24ms/step - reward: -14.4860
13 episodes - episode_reward: -4084.180 [-4314.205, -3921.720] - loss: 1382.571 - mean_squared_error: 2765.142 - mean_q: -1366.616 - reward_fwd: -14.465 - reward_ctrl: -0.021

Interval 81 (800000 steps performed)
10000/10000 [==============================] - 260s 26ms/step - reward: -16.2572
12 episodes - episode_reward: -4227.322 [-4495.896, -4010.193] - loss: 1473.702 - mean_squared_error: 2947.404 - mean_q: -1362.950 - reward_fwd: -16.234 - reward_ctrl: -0.023

Interval 82 (810000 steps performed)
10000/10000 [==============================] - 249s 25ms/step - reward: -15.6765
13 episodes - episode_reward: -4153.795 [-4457.753, -4030.056] - loss: 1399.510 - mean_squared_error: 2799.021 - mean_q: -1357.038 - reward_fwd: -15.656 - reward_ctrl: -0.020

Interval 83 (820000 steps performed)
10000/10000 [==============================] - 255s 26ms/step - reward: -16.4895
12 episodes - episode_reward: -4171.042 [-4312.158, -4011.831] - loss: 1032.917 - mean_squared_error: 2065.834 - mean_q: -1352.481 - reward_fwd: -16.468 - reward_ctrl: -0.022

Interval 84 (830000 steps performed)
10000/10000 [==============================] - 262s 26ms/step - reward: -14.0935
13 episodes - episode_reward: -4043.887 [-4190.945, -3979.382] - loss: 1211.980 - mean_squared_error: 2423.961 - mean_q: -1349.177 - reward_fwd: -14.072 - reward_ctrl: -0.021

Interval 85 (840000 steps performed)
10000/10000 [==============================] - 251s 25ms/step - reward: -16.8238
12 episodes - episode_reward: -4130.232 [-4236.694, -3993.781] - loss: 1496.490 - mean_squared_error: 2992.979 - mean_q: -1343.354 - reward_fwd: -16.804 - reward_ctrl: -0.019

Interval 86 (850000 steps performed)
10000/10000 [==============================] - 265s 27ms/step - reward: -15.9949
13 episodes - episode_reward: -4225.640 [-4408.090, -3975.500] - loss: 1301.378 - mean_squared_error: 2602.756 - mean_q: -1337.931 - reward_fwd: -15.974 - reward_ctrl: -0.021

Interval 87 (860000 steps performed)
10000/10000 [==============================] - 272s 27ms/step - reward: -17.8477
12 episodes - episode_reward: -4175.548 [-4464.909, -3975.415] - loss: 1246.493 - mean_squared_error: 2492.987 - mean_q: -1338.152 - reward_fwd: -17.828 - reward_ctrl: -0.020

Interval 88 (870000 steps performed)
10000/10000 [==============================] - 259s 26ms/step - reward: -16.4211
13 episodes - episode_reward: -4138.339 [-4327.610, -3912.324] - loss: 1356.888 - mean_squared_error: 2713.775 - mean_q: -1336.181 - reward_fwd: -16.404 - reward_ctrl: -0.018

Interval 89 (880000 steps performed)
10000/10000 [==============================] - 253s 25ms/step - reward: -19.7889
12 episodes - episode_reward: -4245.375 [-4630.458, -4019.158] - loss: 1273.317 - mean_squared_error: 2546.634 - mean_q: -1333.283 - reward_fwd: -19.774 - reward_ctrl: -0.015

Interval 90 (890000 steps performed)
10000/10000 [==============================] - 271s 27ms/step - reward: -19.1580
13 episodes - episode_reward: -4193.921 [-4370.010, -4018.144] - loss: 1306.807 - mean_squared_error: 2613.614 - mean_q: -1337.689 - reward_fwd: -19.144 - reward_ctrl: -0.014

Interval 91 (900000 steps performed)
10000/10000 [==============================] - 283s 28ms/step - reward: -16.9909
12 episodes - episode_reward: -4087.939 [-4341.893, -3970.988] - loss: 1327.298 - mean_squared_error: 2654.596 - mean_q: -1337.120 - reward_fwd: -16.975 - reward_ctrl: -0.016

Interval 92 (910000 steps performed)
10000/10000 [==============================] - 288s 29ms/step - reward: -16.4578
13 episodes - episode_reward: -4153.476 [-4338.019, -3995.937] - loss: 1278.828 - mean_squared_error: 2557.656 - mean_q: -1334.408 - reward_fwd: -16.441 - reward_ctrl: -0.017

Interval 93 (920000 steps performed)
10000/10000 [==============================] - 278s 28ms/step - reward: -21.1838
12 episodes - episode_reward: -4321.797 [-4537.022, -4183.764] - loss: 1289.380 - mean_squared_error: 2578.761 - mean_q: -1326.756 - reward_fwd: -21.168 - reward_ctrl: -0.016

Interval 94 (930000 steps performed)
10000/10000 [==============================] - 276s 28ms/step - reward: -16.5572
13 episodes - episode_reward: -4140.216 [-4486.117, -3935.658] - loss: 1148.163 - mean_squared_error: 2296.326 - mean_q: -1323.114 - reward_fwd: -16.540 - reward_ctrl: -0.018

Interval 95 (940000 steps performed)
10000/10000 [==============================] - 281s 28ms/step - reward: -16.2913
12 episodes - episode_reward: -4101.932 [-4325.583, -3956.806] - loss: 1186.239 - mean_squared_error: 2372.479 - mean_q: -1326.417 - reward_fwd: -16.272 - reward_ctrl: -0.019

Interval 96 (950000 steps performed)
10000/10000 [==============================] - 279s 28ms/step - reward: -16.9761
13 episodes - episode_reward: -4203.925 [-4326.811, -4042.997] - loss: 1417.397 - mean_squared_error: 2834.794 - mean_q: -1324.858 - reward_fwd: -16.960 - reward_ctrl: -0.016

Interval 97 (960000 steps performed)
10000/10000 [==============================] - 302s 30ms/step - reward: -15.6358
12 episodes - episode_reward: -4055.642 [-4149.991, -3888.272] - loss: 1348.063 - mean_squared_error: 2696.125 - mean_q: -1333.808 - reward_fwd: -15.617 - reward_ctrl: -0.019

Interval 98 (970000 steps performed)
10000/10000 [==============================] - 295s 30ms/step - reward: -16.1504
13 episodes - episode_reward: -4111.343 [-4379.699, -3993.710] - loss: 1108.086 - mean_squared_error: 2216.172 - mean_q: -1338.050 - reward_fwd: -16.131 - reward_ctrl: -0.020

Interval 99 (980000 steps performed)
10000/10000 [==============================] - 311s 31ms/step - reward: -19.9195
12 episodes - episode_reward: -4211.534 [-4389.110, -4062.590] - loss: 1298.310 - mean_squared_error: 2596.620 - mean_q: -1340.262 - reward_fwd: -19.902 - reward_ctrl: -0.017

Interval 100 (990000 steps performed)
10000/10000 [==============================] - 309s 31ms/step - reward: -20.3202
done, took 18735.477 seconds

reward_fwd = -80*((xposafter-0.6)**2+ (yposafter+0.6)**2) #(xposafter - xposbefore) / self.dt
reward_ctrl = - ctrl_cost_coeff * np.square(a).sum()
reward = reward_fwd + reward_ctrl

only 8 states
