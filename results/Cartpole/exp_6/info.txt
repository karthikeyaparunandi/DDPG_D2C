Using TensorFlow backend.
running build_ext
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
state_input (InputLayer)     (None, 1, 4)              0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 400)               2000      
_________________________________________________________________
activation_1 (Activation)    (None, 400)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 300)               120300    
_________________________________________________________________
activation_2 (Activation)    (None, 300)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 301       
_________________________________________________________________
activation_3 (Activation)    (None, 1)                 0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 1)                 0         
=================================================================
Total params: 122,601
Trainable params: 122,601
Non-trainable params: 0
_________________________________________________________________
None
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
observation_input (InputLayer)  (None, 1, 4)         0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 4)            0           observation_input[0][0]          
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 5)            0           action_input[0][0]               
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 400)          2400        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 400)          0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 300)          120300      activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 300)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            301         activation_5[0][0]               
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 1)            0           dense_6[0][0]                    
==================================================================================================
Total params: 123,001
Trainable params: 123,001
Non-trainable params: 0
__________________________________________________________________________________________________
None
2019-02-09 03:51:54.113258: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Training for 500000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 60s 6ms/step - reward: -306.1632
333 episodes - episode_reward: -7911.948 [-12596.593, -6953.980] - loss: 24454.890 - mean_squared_error: 48909.779 - mean_q: -1305.205

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 61s 6ms/step - reward: -296.8126
333 episodes - episode_reward: -7680.944 [-9145.686, -6932.117] - loss: 87665.898 - mean_squared_error: 175331.797 - mean_q: -2982.204

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 63s 6ms/step - reward: -372.8970
334 episodes - episode_reward: -9580.525 [-19404.565, -7615.254] - loss: 119665.344 - mean_squared_error: 239330.688 - mean_q: -3676.135

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 65s 7ms/step - reward: -334.5850
333 episodes - episode_reward: -8635.495 [-15333.379, -6973.840] - loss: 157478.969 - mean_squared_error: 314957.938 - mean_q: -4222.435

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 69s 7ms/step - reward: -344.3326
333 episodes - episode_reward: -8882.542 [-12274.624, -7552.829] - loss: 174092.359 - mean_squared_error: 348184.719 - mean_q: -4646.579

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 69s 7ms/step - reward: -354.0084
334 episodes - episode_reward: -9146.633 [-17662.305, -5083.627] - loss: 182549.422 - mean_squared_error: 365098.844 - mean_q: -4908.797

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 72s 7ms/step - reward: -320.8469
333 episodes - episode_reward: -8291.770 [-10080.371, -6805.343] - loss: 185642.891 - mean_squared_error: 371285.781 - mean_q: -4757.751

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 75s 8ms/step - reward: -321.0384
333 episodes - episode_reward: -8351.642 [-11143.200, -6424.996] - loss: 180283.578 - mean_squared_error: 360567.156 - mean_q: -4544.214

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 78s 8ms/step - reward: -334.8805
334 episodes - episode_reward: -8718.847 [-12682.663, -7007.063] - loss: 182604.953 - mean_squared_error: 365209.906 - mean_q: -4469.235

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 81s 8ms/step - reward: -384.6069
333 episodes - episode_reward: -10041.763 [-20048.847, -7352.499] - loss: 194704.266 - mean_squared_error: 389408.531 - mean_q: -4640.626

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: -363.8312
333 episodes - episode_reward: -9523.946 [-24140.697, -7303.844] - loss: 217203.484 - mean_squared_error: 434406.969 - mean_q: -4751.881

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: -300.1778
334 episodes - episode_reward: -7867.312 [-25647.187, -4391.918] - loss: 226224.125 - mean_squared_error: 452448.250 - mean_q: -4716.803

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: -314.9386
333 episodes - episode_reward: -8206.570 [-19126.915, -4307.493] - loss: 236686.281 - mean_squared_error: 473372.562 - mean_q: -4637.308

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: -260.5814
333 episodes - episode_reward: -6942.816 [-10451.681, -4188.544] - loss: 226756.656 - mean_squared_error: 453513.312 - mean_q: -4525.214

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 95s 9ms/step - reward: -203.9704
334 episodes - episode_reward: -5574.070 [-14127.593, -4198.725] - loss: 192085.062 - mean_squared_error: 384170.125 - mean_q: -4126.834

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 101s 10ms/step - reward: -174.1707
333 episodes - episode_reward: -4841.064 [-10519.838, -4280.973] - loss: 156482.969 - mean_squared_error: 312965.938 - mean_q: -3509.535

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 95s 10ms/step - reward: -168.6564
333 episodes - episode_reward: -4726.082 [-10969.130, -4391.378] - loss: 122199.461 - mean_squared_error: 244398.922 - mean_q: -2865.281

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 99s 10ms/step - reward: -165.8893
334 episodes - episode_reward: -4663.046 [-6058.373, -4370.166] - loss: 94918.039 - mean_squared_error: 189836.078 - mean_q: -2406.613

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 102s 10ms/step - reward: -164.8264
333 episodes - episode_reward: -4633.868 [-5200.449, -4327.707] - loss: 87212.602 - mean_squared_error: 174425.203 - mean_q: -2245.192

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 105s 11ms/step - reward: -164.4936
333 episodes - episode_reward: -4633.576 [-5171.762, -4409.446] - loss: 84021.188 - mean_squared_error: 168042.375 - mean_q: -2207.731

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 109s 11ms/step - reward: -166.9420
334 episodes - episode_reward: -4703.555 [-5262.869, -4400.214] - loss: 75294.039 - mean_squared_error: 150588.078 - mean_q: -2192.375

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 112s 11ms/step - reward: -164.5604
333 episodes - episode_reward: -4631.061 [-5023.745, -4414.923] - loss: 70550.289 - mean_squared_error: 141100.578 - mean_q: -2147.978

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 115s 12ms/step - reward: -163.9953
333 episodes - episode_reward: -4627.090 [-5069.015, -4433.013] - loss: 66219.938 - mean_squared_error: 132439.875 - mean_q: -2089.073

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 118s 12ms/step - reward: -162.4334
334 episodes - episode_reward: -4582.848 [-5410.871, -4354.671] - loss: 66188.891 - mean_squared_error: 132377.781 - mean_q: -2074.516

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 122s 12ms/step - reward: -162.3666
333 episodes - episode_reward: -4573.116 [-5184.246, -4391.902] - loss: 65524.371 - mean_squared_error: 131048.742 - mean_q: -2061.508

Interval 26 (250000 steps performed)
10000/10000 [==============================] - 125s 12ms/step - reward: -160.8577
333 episodes - episode_reward: -4542.696 [-5126.337, -4396.629] - loss: 62637.832 - mean_squared_error: 125275.664 - mean_q: -2022.750

Interval 27 (260000 steps performed)
10000/10000 [==============================] - 128s 13ms/step - reward: -160.6098
334 episodes - episode_reward: -4537.452 [-4955.232, -4393.858] - loss: 59968.473 - mean_squared_error: 119936.945 - mean_q: -2022.498

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 130s 13ms/step - reward: -159.4291
333 episodes - episode_reward: -4490.972 [-5397.818, -4330.806] - loss: 56044.895 - mean_squared_error: 112089.789 - mean_q: -1994.037

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 134s 13ms/step - reward: -158.3457
333 episodes - episode_reward: -4470.429 [-5260.485, -4328.839] - loss: 55415.742 - mean_squared_error: 110831.484 - mean_q: -1957.190

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 137s 14ms/step - reward: -159.1105
334 episodes - episode_reward: -4488.724 [-5412.718, -4267.791] - loss: 52060.719 - mean_squared_error: 104121.438 - mean_q: -1922.326

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 141s 14ms/step - reward: -158.5595
333 episodes - episode_reward: -4466.317 [-5033.501, -4311.559] - loss: 50276.688 - mean_squared_error: 100553.375 - mean_q: -1880.570

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 143s 14ms/step - reward: -157.9833
333 episodes - episode_reward: -4460.629 [-4945.545, -4302.960] - loss: 50026.984 - mean_squared_error: 100053.969 - mean_q: -1856.087

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 147s 15ms/step - reward: -158.7010
334 episodes - episode_reward: -4478.581 [-5295.836, -4298.424] - loss: 48913.500 - mean_squared_error: 97827.000 - mean_q: -1848.357

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 150s 15ms/step - reward: -159.2574
333 episodes - episode_reward: -4484.186 [-5527.787, -4292.489] - loss: 48637.195 - mean_squared_error: 97274.391 - mean_q: -1821.859

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 153s 15ms/step - reward: -156.3266
333 episodes - episode_reward: -4423.121 [-4816.663, -4307.539] - loss: 47622.328 - mean_squared_error: 95244.656 - mean_q: -1803.221

Interval 36 (350000 steps performed)
10000/10000 [==============================] - 157s 16ms/step - reward: -156.5232
334 episodes - episode_reward: -4428.824 [-5800.188, -4284.919] - loss: 46650.203 - mean_squared_error: 93300.406 - mean_q: -1787.141

Interval 37 (360000 steps performed)
10000/10000 [==============================] - 160s 16ms/step - reward: -156.4307
333 episodes - episode_reward: -4412.827 [-4805.037, -4269.342] - loss: 45000.332 - mean_squared_error: 90000.664 - mean_q: -1769.375

Interval 38 (370000 steps performed)
10000/10000 [==============================] - 163s 16ms/step - reward: -155.9718
333 episodes - episode_reward: -4410.402 [-5045.128, -4298.251] - loss: 43484.949 - mean_squared_error: 86969.898 - mean_q: -1752.194

Interval 39 (380000 steps performed)
10000/10000 [==============================] - 167s 17ms/step - reward: -154.5839
334 episodes - episode_reward: -4372.391 [-4773.515, -4259.744] - loss: 43206.863 - mean_squared_error: 86413.727 - mean_q: -1740.307

Interval 40 (390000 steps performed)
10000/10000 [==============================] - 169s 17ms/step - reward: -154.7164
333 episodes - episode_reward: -4365.692 [-5626.575, -4227.846] - loss: 43094.410 - mean_squared_error: 86188.820 - mean_q: -1700.315

Interval 41 (400000 steps performed)
10000/10000 [==============================] - 173s 17ms/step - reward: -153.0656
333 episodes - episode_reward: -4330.884 [-5076.603, -4224.204] - loss: 40635.523 - mean_squared_error: 81271.047 - mean_q: -1681.468

Interval 42 (410000 steps performed)
10000/10000 [==============================] - 177s 18ms/step - reward: -153.8245
334 episodes - episode_reward: -4353.735 [-4764.513, -4243.059] - loss: 40737.438 - mean_squared_error: 81474.875 - mean_q: -1680.980

Interval 43 (420000 steps performed)
10000/10000 [==============================] - 180s 18ms/step - reward: -155.4135
333 episodes - episode_reward: -4385.366 [-4629.359, -4268.903] - loss: 42661.004 - mean_squared_error: 85322.008 - mean_q: -1672.412

Interval 44 (430000 steps performed)
10000/10000 [==============================] - 184s 18ms/step - reward: -155.0353
333 episodes - episode_reward: -4384.781 [-4677.302, -4286.715] - loss: 39825.465 - mean_squared_error: 79650.930 - mean_q: -1640.739

Interval 45 (440000 steps performed)
10000/10000 [==============================] - 187s 19ms/step - reward: -154.9086
334 episodes - episode_reward: -4381.019 [-4753.017, -4236.220] - loss: 38026.129 - mean_squared_error: 76052.258 - mean_q: -1641.148

Interval 46 (450000 steps performed)
10000/10000 [==============================] - 191s 19ms/step - reward: -154.1169
333 episodes - episode_reward: -4351.252 [-5502.870, -4226.026] - loss: 37595.145 - mean_squared_error: 75190.289 - mean_q: -1619.439

Interval 47 (460000 steps performed)
10000/10000 [==============================] - 194s 19ms/step - reward: -154.1006
333 episodes - episode_reward: -4362.113 [-4707.027, -4253.226] - loss: 34745.641 - mean_squared_error: 69491.281 - mean_q: -1605.554

Interval 48 (470000 steps performed)
10000/10000 [==============================] - 197s 20ms/step - reward: -153.5883
334 episodes - episode_reward: -4351.743 [-5662.621, -4248.374] - loss: 35113.496 - mean_squared_error: 70226.992 - mean_q: -1596.162

Interval 49 (480000 steps performed)
10000/10000 [==============================] - 200s 20ms/step - reward: -154.2977
333 episodes - episode_reward: -4358.899 [-4917.128, -4245.461] - loss: 33112.168 - mean_squared_error: 66224.336 - mean_q: -1569.954

Interval 50 (490000 steps performed)
10000/10000 [==============================] - 200s 20ms/step - reward: -152.7825
done, took 6306.695 seconds
Creating window glfw
-4328.59340840797 [[ 0.00000000e+00  3.14159265e+00  0.00000000e+00  0.00000000e+00]
 [ 5.83784033e-02 -3.05717109e+00  1.16627819e+00  1.66505109e+00]
 [ 2.23771748e-01 -2.83094295e+00  2.13651132e+00  2.78925556e+00]
 [ 4.21809337e-01 -2.60073729e+00  1.82375737e+00  1.77188392e+00]
 [ 5.76381786e-01 -2.49720529e+00  1.26910849e+00  2.87917689e-01]
 [ 6.74377351e-01 -2.54528465e+00  6.90037038e-01 -1.24494309e+00]
 [ 6.84016612e-01 -2.78106305e+00 -5.01023076e-01 -3.45691686e+00]
 [ 5.77326223e-01  3.05994772e+00 -1.62436675e+00 -5.28766918e+00]
 [ 3.59888174e-01  2.47328565e+00 -2.69803913e+00 -6.22971657e+00]
 [ 7.59590018e-02  1.89146000e+00 -2.97902566e+00 -5.29408801e+00]
 [-1.73430262e-01  1.44387707e+00 -2.02412554e+00 -3.75431042e+00]
 [-3.20148519e-01  1.11852344e+00 -9.16171894e-01 -2.85899388e+00]
 [-3.56218275e-01  8.52077218e-01  1.94830648e-01 -2.56723877e+00]
 [-2.83828016e-01  5.89422510e-01  1.25601740e+00 -2.78268546e+00]
 [-1.56902083e-01  3.44015694e-01  1.28206615e+00 -2.17869521e+00]
 [-4.86750215e-02  1.74054363e-01  8.82733198e-01 -1.25654958e+00]
 [ 2.79543334e-02  7.50897374e-02  6.50734971e-01 -7.47261792e-01]
 [ 9.05144253e-02  7.74934745e-03  6.01225998e-01 -6.16980627e-01]
 [ 1.38036383e-01 -3.61942769e-02  3.49738509e-01 -2.73467361e-01]
 [ 1.60171801e-01 -4.78204117e-02  9.31188145e-02  3.77784604e-02]
 [ 1.74275172e-01 -5.45168465e-02  1.89021276e-01 -1.73363003e-01]
 [ 1.71769151e-01 -4.45588533e-02 -2.39255632e-01  3.74977927e-01]
 [ 1.65058096e-01 -3.48886293e-02  1.04917176e-01 -1.78923546e-01]
 [ 1.56720055e-01 -2.78646326e-02 -2.71753528e-01  3.21079089e-01]
 [ 1.47628522e-01 -2.37994972e-02  8.98658660e-02 -2.38546000e-01]
 [ 1.35122813e-01 -1.81229277e-02 -3.40036480e-01  3.53362338e-01]
 [ 1.16613288e-01 -6.21284389e-03 -3.03013272e-02 -1.11923403e-01]
 [ 1.00723636e-01  9.27732415e-04 -2.87568428e-01  2.56466363e-01]
 [ 9.09084012e-02 -8.07892113e-04  9.12728113e-02 -2.91442879e-01]
 [ 8.01975322e-02 -1.31680749e-03 -3.05472351e-01  2.80936699e-01]
 [ 6.74359309e-02  1.00253239e-03  5.02024055e-02 -2.33771664e-01]]

reward -         reward = -10*(2*ob[0]**2 + 3*ob[1]**2  + 1.2*ob[2]**2 + 1.2*ob[3]**2) 

