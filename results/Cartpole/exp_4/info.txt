Using TensorFlow backend.
running build_ext
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
state_input (InputLayer)     (None, 1, 4)              0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 400)               2000      
_________________________________________________________________
activation_1 (Activation)    (None, 400)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 300)               120300    
_________________________________________________________________
activation_2 (Activation)    (None, 300)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 301       
_________________________________________________________________
activation_3 (Activation)    (None, 1)                 0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 1)                 0         
=================================================================
Total params: 122,601
Trainable params: 122,601
Non-trainable params: 0
_________________________________________________________________
None
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
observation_input (InputLayer)  (None, 1, 4)         0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 4)            0           observation_input[0][0]          
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 5)            0           action_input[0][0]               
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 400)          2400        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 400)          0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 300)          120300      activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 300)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            301         activation_5[0][0]               
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 1)            0           dense_6[0][0]                    
==================================================================================================
Total params: 123,001
Trainable params: 123,001
Non-trainable params: 0
__________________________________________________________________________________________________
None
2019-02-08 20:14:33.754248: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Training for 350000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 62s 6ms/step - reward: -308.5965
333 episodes - episode_reward: -7982.133 [-17119.420, -7031.414] - loss: 24122.488 - mean_squared_error: 48244.976 - mean_q: -1364.190

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 62s 6ms/step - reward: -319.0782
333 episodes - episode_reward: -8225.593 [-10493.721, -7090.097] - loss: 71429.305 - mean_squared_error: 142858.609 - mean_q: -2921.735

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 64s 6ms/step - reward: -349.7202
334 episodes - episode_reward: -9016.701 [-11505.241, -7755.812] - loss: 100738.070 - mean_squared_error: 201476.141 - mean_q: -3501.630

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 66s 7ms/step - reward: -344.1854
333 episodes - episode_reward: -8926.939 [-13158.696, -6919.848] - loss: 125059.266 - mean_squared_error: 250118.531 - mean_q: -3974.848

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 68s 7ms/step - reward: -434.4532
333 episodes - episode_reward: -11137.268 [-21068.582, -6479.353] - loss: 162992.562 - mean_squared_error: 325985.125 - mean_q: -4230.497

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 71s 7ms/step - reward: -328.7211
334 episodes - episode_reward: -8478.579 [-9381.249, -7014.789] - loss: 184699.125 - mean_squared_error: 369398.250 - mean_q: -4607.082

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 73s 7ms/step - reward: -353.2792
333 episodes - episode_reward: -9164.357 [-13436.244, -6820.713] - loss: 177357.984 - mean_squared_error: 354715.969 - mean_q: -4628.218

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 76s 8ms/step - reward: -360.7825
333 episodes - episode_reward: -9390.126 [-16740.056, -5671.760] - loss: 168569.438 - mean_squared_error: 337138.875 - mean_q: -4378.678

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 79s 8ms/step - reward: -411.8005
334 episodes - episode_reward: -10683.789 [-16652.011, -5589.452] - loss: 161698.875 - mean_squared_error: 323397.750 - mean_q: -4348.087

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 82s 8ms/step - reward: -410.2584
333 episodes - episode_reward: -10501.972 [-17332.248, -5557.318] - loss: 195706.453 - mean_squared_error: 391412.906 - mean_q: -4514.610

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: -333.7315
333 episodes - episode_reward: -8681.833 [-13765.430, -5238.130] - loss: 229637.406 - mean_squared_error: 459274.812 - mean_q: -4762.048

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: -371.5704
334 episodes - episode_reward: -9705.538 [-13196.005, -6594.113] - loss: 212178.203 - mean_squared_error: 424356.406 - mean_q: -4883.659

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: -360.2740
333 episodes - episode_reward: -9433.743 [-12305.666, -4951.982] - loss: 205363.531 - mean_squared_error: 410727.062 - mean_q: -4886.220

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: -279.5076
333 episodes - episode_reward: -7442.636 [-11988.809, -5079.951] - loss: 203147.000 - mean_squared_error: 406294.000 - mean_q: -4853.924

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 96s 10ms/step - reward: -286.5771
334 episodes - episode_reward: -7648.238 [-12367.992, -5330.133] - loss: 190097.047 - mean_squared_error: 380194.094 - mean_q: -4603.567

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 99s 10ms/step - reward: -239.9291
333 episodes - episode_reward: -6549.024 [-11061.897, -5289.736] - loss: 172980.141 - mean_squared_error: 345960.281 - mean_q: -4345.533

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 103s 10ms/step - reward: -200.4422
333 episodes - episode_reward: -5596.117 [-7492.658, -4859.118] - loss: 157285.359 - mean_squared_error: 314570.719 - mean_q: -4096.536

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 107s 11ms/step - reward: -188.7450
334 episodes - episode_reward: -5307.095 [-10343.651, -4802.242] - loss: 133586.016 - mean_squared_error: 267172.031 - mean_q: -3740.290

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 110s 11ms/step - reward: -184.5302
333 episodes - episode_reward: -5180.878 [-10593.440, -4783.904] - loss: 112802.469 - mean_squared_error: 225604.938 - mean_q: -3414.994

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 113s 11ms/step - reward: -186.2840
333 episodes - episode_reward: -5221.450 [-6704.174, -4767.423] - loss: 101875.867 - mean_squared_error: 203751.734 - mean_q: -3160.780

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 121s 12ms/step - reward: -189.6550
334 episodes - episode_reward: -5287.829 [-7068.211, -4746.252] - loss: 91567.070 - mean_squared_error: 183134.141 - mean_q: -2959.238

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 119s 12ms/step - reward: -179.3864
333 episodes - episode_reward: -5020.753 [-5974.321, -4699.561] - loss: 84933.469 - mean_squared_error: 169866.938 - mean_q: -2841.275

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 121s 12ms/step - reward: -174.6114
333 episodes - episode_reward: -4909.677 [-12452.047, -4700.237] - loss: 78163.680 - mean_squared_error: 156327.359 - mean_q: -2745.143

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 125s 12ms/step - reward: -173.8795
334 episodes - episode_reward: -4894.303 [-8272.909, -4691.115] - loss: 74171.523 - mean_squared_error: 148343.047 - mean_q: -2673.377

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 128s 13ms/step - reward: -174.0061
333 episodes - episode_reward: -4889.179 [-6239.028, -4719.499] - loss: 72266.398 - mean_squared_error: 144532.797 - mean_q: -2616.767

Interval 26 (250000 steps performed)
10000/10000 [==============================] - 133s 13ms/step - reward: -171.4638
333 episodes - episode_reward: -4835.323 [-8632.825, -4705.026] - loss: 69834.484 - mean_squared_error: 139668.969 - mean_q: -2521.693

Interval 27 (260000 steps performed)
10000/10000 [==============================] - 136s 14ms/step - reward: -172.4119
334 episodes - episode_reward: -4860.086 [-8873.514, -4712.281] - loss: 61656.219 - mean_squared_error: 123312.438 - mean_q: -2447.885

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 145s 15ms/step - reward: -173.4606
333 episodes - episode_reward: -4875.955 [-8764.128, -4718.618] - loss: 60395.668 - mean_squared_error: 120791.336 - mean_q: -2397.023

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 143s 14ms/step - reward: -173.2376
333 episodes - episode_reward: -4872.868 [-9447.875, -4709.482] - loss: 57816.711 - mean_squared_error: 115633.422 - mean_q: -2335.607

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 146s 15ms/step - reward: -171.8006
334 episodes - episode_reward: -4834.929 [-5865.517, -4692.685] - loss: 53494.223 - mean_squared_error: 106988.445 - mean_q: -2294.680

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 149s 15ms/step - reward: -176.0300
333 episodes - episode_reward: -4925.324 [-6163.456, -4710.644] - loss: 55680.219 - mean_squared_error: 111360.438 - mean_q: -2284.279

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 152s 15ms/step - reward: -172.2500
333 episodes - episode_reward: -4842.163 [-7151.159, -4688.183] - loss: 55294.898 - mean_squared_error: 110589.797 - mean_q: -2256.156

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 155s 15ms/step - reward: -177.8731
334 episodes - episode_reward: -4976.133 [-7635.881, -4692.043] - loss: 47781.172 - mean_squared_error: 95562.344 - mean_q: -2188.277

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 166s 17ms/step - reward: -182.0863
333 episodes - episode_reward: -5077.365 [-9474.292, -4694.783] - loss: 48162.969 - mean_squared_error: 96325.938 - mean_q: -2136.703

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 159s 16ms/step - reward: -173.8107
done, took 3784.996 seconds
Creating window glfw
-4824.552677330391 [[ 0.          3.14159265  0.          0.        ]
 [ 0.0583784  -3.05717109  1.16627819  1.66505109]
 [ 0.1976546  -2.86821273  1.61614526  2.06203857]
 [ 0.35806483 -2.6875001   1.59068253  1.5109251 ]
 [ 0.50202658 -2.59000511  1.28897257  0.42282324]
 [ 0.58192576 -2.646379    0.30795879 -1.54665379]
 [ 0.55380205 -2.90910249 -0.87215978 -3.67830691]
 [ 0.41035364  2.91803354 -1.98426955 -5.31791026]
 [ 0.15853888  2.34360955 -3.02838298 -5.95553176]
 [-0.16172679  1.79370935 -3.37954895 -4.94098895]
 [-0.45812266  1.37535181 -2.56199959 -3.50953354]
 [-0.66410583  1.07090258 -1.56256587 -2.67347364]
 [-0.78045103  0.83127872 -0.7652964  -2.1906822 ]
 [-0.83268741  0.63611388 -0.27945954 -1.76354263]
 [-0.85041904  0.48675244 -0.07498977 -1.25867515]
 [-0.84857143  0.37967824  0.112581   -0.90939464]
 [-0.83020254  0.30373716  0.25541709 -0.62861112]
 [-0.79398874  0.24599057  0.46947586 -0.54153112]
 [-0.74145988  0.20021982  0.58158081 -0.38572391]
 [-0.67565881  0.16407062  0.73485362 -0.34674027]
 [-0.59560736  0.13083269  0.86656029 -0.32671758]
 [-0.50204269  0.09670437  1.00513563 -0.36478183]
 [-0.39615924  0.05844668  1.11298322 -0.41034698]
 [-0.28198105  0.01641452  1.17107132 -0.44121637]
 [-0.1648308  -0.02763329  1.1724464  -0.45117227]
 [-0.05119667 -0.07049225  1.10073919 -0.41717501]
 [ 0.04791127 -0.10213179  0.88181272 -0.22404764]
 [ 0.12134347 -0.11075289  0.58694791  0.0492188 ]
 [ 0.1680013  -0.09601608  0.34602998  0.24936095]
 [ 0.19838612 -0.07122761  0.26137552  0.25286794]
 [ 0.22401931 -0.04976343  0.25103778  0.18200454]]


reward:        reward = -10*(2*ob[0]**2 + 3*ob[1]**2  + 1.2*ob[2]**2 + 1.2*ob[3]**2) - 0.5*a[0]**2
no terminal reward

