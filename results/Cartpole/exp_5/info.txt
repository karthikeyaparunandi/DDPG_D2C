Using TensorFlow backend.
running build_ext
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
state_input (InputLayer)     (None, 1, 4)              0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 400)               2000      
_________________________________________________________________
activation_1 (Activation)    (None, 400)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 300)               120300    
_________________________________________________________________
activation_2 (Activation)    (None, 300)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 301       
_________________________________________________________________
activation_3 (Activation)    (None, 1)                 0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 1)                 0         
=================================================================
Total params: 122,601
Trainable params: 122,601
Non-trainable params: 0
_________________________________________________________________
None
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
observation_input (InputLayer)  (None, 1, 4)         0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 4)            0           observation_input[0][0]          
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 5)            0           action_input[0][0]               
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 400)          2400        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 400)          0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 300)          120300      activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 300)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            301         activation_5[0][0]               
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 1)            0           dense_6[0][0]                    
==================================================================================================
Total params: 123,001
Trainable params: 123,001
Non-trainable params: 0
__________________________________________________________________________________________________
None
2019-02-09 01:14:02.539860: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Training for 500000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 72s 7ms/step - reward: -318.0006
333 episodes - episode_reward: -8223.133 [-11232.015, -7022.066] - loss: 23634.895 - mean_squared_error: 47269.789 - mean_q: -1370.210

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 71s 7ms/step - reward: -278.9354
333 episodes - episode_reward: -7271.307 [-9415.614, -6966.463] - loss: 102478.922 - mean_squared_error: 204957.844 - mean_q: -2942.676

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 72s 7ms/step - reward: -295.1114
334 episodes - episode_reward: -7679.253 [-10607.465, -6977.982] - loss: 155822.578 - mean_squared_error: 311645.156 - mean_q: -3668.788

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 73s 7ms/step - reward: -349.9020
333 episodes - episode_reward: -9052.185 [-11506.510, -7668.655] - loss: 181089.844 - mean_squared_error: 362179.688 - mean_q: -3889.202

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 73s 7ms/step - reward: -340.9079
333 episodes - episode_reward: -8847.633 [-17963.651, -7070.362] - loss: 197207.875 - mean_squared_error: 394415.750 - mean_q: -4130.561

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 75s 7ms/step - reward: -390.5836
334 episodes - episode_reward: -10109.486 [-23807.824, -7311.560] - loss: 251347.562 - mean_squared_error: 502695.125 - mean_q: -4598.404

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 78s 8ms/step - reward: -326.2057
333 episodes - episode_reward: -8476.623 [-9996.981, -7406.867] - loss: 287093.156 - mean_squared_error: 574186.312 - mean_q: -4898.342

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 81s 8ms/step - reward: -335.6038
333 episodes - episode_reward: -8714.172 [-9891.023, -7418.179] - loss: 304829.656 - mean_squared_error: 609659.312 - mean_q: -4925.189

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: -346.4740
334 episodes - episode_reward: -9030.724 [-13227.254, -7668.640] - loss: 282292.562 - mean_squared_error: 564585.125 - mean_q: -4769.686

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: -349.4829
333 episodes - episode_reward: -9050.871 [-12353.986, -6959.727] - loss: 269441.688 - mean_squared_error: 538883.375 - mean_q: -4851.787

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: -452.9829
333 episodes - episode_reward: -11540.950 [-16677.642, -7092.720] - loss: 240410.984 - mean_squared_error: 480821.969 - mean_q: -4601.680

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: -373.4635
334 episodes - episode_reward: -9745.016 [-13347.295, -8512.527] - loss: 223554.812 - mean_squared_error: 447109.625 - mean_q: -4454.086

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: -360.0520
333 episodes - episode_reward: -9357.822 [-10394.476, -7550.554] - loss: 205522.062 - mean_squared_error: 411044.125 - mean_q: -4413.959

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: -356.0325
333 episodes - episode_reward: -9239.290 [-10089.964, -7764.419] - loss: 218134.297 - mean_squared_error: 436268.594 - mean_q: -4582.272

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: -363.4820
334 episodes - episode_reward: -9501.716 [-15275.079, -7607.958] - loss: 214643.359 - mean_squared_error: 429286.719 - mean_q: -4786.685

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 97s 10ms/step - reward: -363.1902
333 episodes - episode_reward: -9464.826 [-15787.014, -5588.083] - loss: 234758.484 - mean_squared_error: 469516.969 - mean_q: -4899.975

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 99s 10ms/step - reward: -356.3348
333 episodes - episode_reward: -9256.059 [-10216.169, -8138.592] - loss: 232543.922 - mean_squared_error: 465087.844 - mean_q: -4853.986

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 102s 10ms/step - reward: -349.9297
334 episodes - episode_reward: -9084.714 [-10142.438, -7311.131] - loss: 241440.797 - mean_squared_error: 482881.594 - mean_q: -4900.680

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 106s 11ms/step - reward: -338.7163
333 episodes - episode_reward: -8853.948 [-15191.393, -5334.230] - loss: 247991.266 - mean_squared_error: 495982.531 - mean_q: -4967.094

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 109s 11ms/step - reward: -362.8942
333 episodes - episode_reward: -9371.573 [-14853.399, -7710.220] - loss: 245613.469 - mean_squared_error: 491226.938 - mean_q: -5023.019

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 113s 11ms/step - reward: -402.5063
334 episodes - episode_reward: -10383.054 [-24783.855, -6065.690] - loss: 233566.547 - mean_squared_error: 467133.094 - mean_q: -4856.296

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 116s 12ms/step - reward: -334.8882
333 episodes - episode_reward: -8804.859 [-18582.786, -5432.876] - loss: 211307.344 - mean_squared_error: 422614.688 - mean_q: -4570.111

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 119s 12ms/step - reward: -278.3771
333 episodes - episode_reward: -7475.575 [-12286.976, -5514.283] - loss: 213876.578 - mean_squared_error: 427753.156 - mean_q: -4556.660

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 123s 12ms/step - reward: -260.5930
334 episodes - episode_reward: -7020.668 [-10549.286, -5229.882] - loss: 222894.625 - mean_squared_error: 445789.250 - mean_q: -4543.082

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 126s 13ms/step - reward: -224.7998
333 episodes - episode_reward: -6191.431 [-9866.654, -5320.970] - loss: 204307.953 - mean_squared_error: 408615.906 - mean_q: -4366.957

Interval 26 (250000 steps performed)
10000/10000 [==============================] - 129s 13ms/step - reward: -212.7451
333 episodes - episode_reward: -5926.498 [-7779.197, -5248.679] - loss: 183450.750 - mean_squared_error: 366901.500 - mean_q: -4031.858

Interval 27 (260000 steps performed)
10000/10000 [==============================] - 132s 13ms/step - reward: -205.6915
334 episodes - episode_reward: -5755.108 [-6436.150, -5293.278] - loss: 165680.922 - mean_squared_error: 331361.844 - mean_q: -3707.746

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 137s 14ms/step - reward: -206.5647
333 episodes - episode_reward: -5771.791 [-7107.729, -5360.533] - loss: 148160.547 - mean_squared_error: 296321.094 - mean_q: -3387.369

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 139s 14ms/step - reward: -204.2333
333 episodes - episode_reward: -5718.570 [-6512.112, -5261.356] - loss: 130118.117 - mean_squared_error: 260236.234 - mean_q: -3115.727

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 143s 14ms/step - reward: -199.0864
334 episodes - episode_reward: -5581.242 [-6171.097, -5205.245] - loss: 110552.578 - mean_squared_error: 221105.156 - mean_q: -2897.954

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 147s 15ms/step - reward: -191.5626
333 episodes - episode_reward: -5370.935 [-5989.129, -5029.790] - loss: 103448.281 - mean_squared_error: 206896.562 - mean_q: -2751.953

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 149s 15ms/step - reward: -186.2192
333 episodes - episode_reward: -5232.690 [-5754.606, -5019.927] - loss: 95834.398 - mean_squared_error: 191668.797 - mean_q: -2676.651

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 152s 15ms/step - reward: -188.3484
334 episodes - episode_reward: -5290.023 [-5904.300, -5026.079] - loss: 89410.320 - mean_squared_error: 178820.641 - mean_q: -2618.677

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 156s 16ms/step - reward: -187.4119
333 episodes - episode_reward: -5259.788 [-5971.295, -5004.598] - loss: 82023.977 - mean_squared_error: 164047.953 - mean_q: -2567.244

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 159s 16ms/step - reward: -186.8884
333 episodes - episode_reward: -5249.344 [-5844.626, -4992.961] - loss: 83200.617 - mean_squared_error: 166401.234 - mean_q: -2548.081

Interval 36 (350000 steps performed)
10000/10000 [==============================] - 162s 16ms/step - reward: -183.6273
334 episodes - episode_reward: -5170.296 [-5647.455, -4936.699] - loss: 80942.742 - mean_squared_error: 161885.484 - mean_q: -2557.187

Interval 37 (360000 steps performed)
10000/10000 [==============================] - 165s 17ms/step - reward: -184.4664
333 episodes - episode_reward: -5179.488 [-5715.457, -4956.130] - loss: 81656.484 - mean_squared_error: 163312.969 - mean_q: -2562.145

Interval 38 (370000 steps performed)
10000/10000 [==============================] - 168s 17ms/step - reward: -184.1455
333 episodes - episode_reward: -5176.153 [-5667.673, -4926.344] - loss: 78692.211 - mean_squared_error: 157384.422 - mean_q: -2554.873

Interval 39 (380000 steps performed)
10000/10000 [==============================] - 173s 17ms/step - reward: -184.4726
334 episodes - episode_reward: -5189.295 [-5846.878, -4871.396] - loss: 75586.375 - mean_squared_error: 151172.750 - mean_q: -2493.167

Interval 40 (390000 steps performed)
10000/10000 [==============================] - 176s 18ms/step - reward: -183.8755
333 episodes - episode_reward: -5163.470 [-5815.901, -4946.478] - loss: 70925.180 - mean_squared_error: 141850.359 - mean_q: -2473.132

Interval 41 (400000 steps performed)
10000/10000 [==============================] - 178s 18ms/step - reward: -184.7907
333 episodes - episode_reward: -5196.841 [-6610.051, -4888.102] - loss: 69678.367 - mean_squared_error: 139356.734 - mean_q: -2441.045

Interval 42 (410000 steps performed)
10000/10000 [==============================] - 183s 18ms/step - reward: -186.5183
334 episodes - episode_reward: -5242.602 [-6187.241, -4914.845] - loss: 67693.469 - mean_squared_error: 135386.938 - mean_q: -2393.589

Interval 43 (420000 steps performed)
10000/10000 [==============================] - 186s 19ms/step - reward: -186.0520
333 episodes - episode_reward: -5209.649 [-5738.104, -4917.992] - loss: 65763.062 - mean_squared_error: 131526.125 - mean_q: -2344.099

Interval 44 (430000 steps performed)
10000/10000 [==============================] - 190s 19ms/step - reward: -183.6806
333 episodes - episode_reward: -5164.122 [-5645.806, -4896.946] - loss: 62168.434 - mean_squared_error: 124336.867 - mean_q: -2306.977

Interval 45 (440000 steps performed)
10000/10000 [==============================] - 193s 19ms/step - reward: -181.6139
334 episodes - episode_reward: -5110.625 [-5523.365, -4860.036] - loss: 60384.223 - mean_squared_error: 120768.445 - mean_q: -2297.761

Interval 46 (450000 steps performed)
10000/10000 [==============================] - 197s 20ms/step - reward: -182.5890
333 episodes - episode_reward: -5126.093 [-5669.327, -4881.908] - loss: 60834.488 - mean_squared_error: 121668.977 - mean_q: -2306.372

Interval 47 (460000 steps performed)
10000/10000 [==============================] - 200s 20ms/step - reward: -179.7504
333 episodes - episode_reward: -5063.988 [-5521.670, -4823.648] - loss: 58865.324 - mean_squared_error: 117730.648 - mean_q: -2267.053

Interval 48 (470000 steps performed)
10000/10000 [==============================] - 204s 20ms/step - reward: -177.9804
334 episodes - episode_reward: -5012.025 [-5418.203, -4822.571] - loss: 62803.480 - mean_squared_error: 125606.961 - mean_q: -2279.630

Interval 49 (480000 steps performed)
10000/10000 [==============================] - 206s 21ms/step - reward: -180.1618
333 episodes - episode_reward: -5060.379 [-7775.099, -4857.842] - loss: 59526.355 - mean_squared_error: 119052.711 - mean_q: -2242.488

Interval 50 (490000 steps performed)
10000/10000 [==============================] - 207s 21ms/step - reward: -178.0784
done, took 6559.815 seconds
Creating window glfw
-4980.929113736836 [[ 0.00000000e+00  3.14159265e+00  0.00000000e+00  0.00000000e+00]
 [ 5.83784033e-02 -3.05717109e+00  1.16627819e+00  1.66505109e+00]
 [ 2.20297839e-01 -2.83589461e+00  2.06733622e+00  2.69286382e+00]
 [ 4.22384783e-01 -2.60019185e+00  1.97297919e+00  1.97082821e+00]
 [ 6.17286205e-01 -2.44654016e+00  1.92555600e+00  1.07428818e+00]
 [ 7.95253044e-01 -2.40252615e+00  1.63442230e+00 -1.99042648e-01]
 [ 9.27730044e-01 -2.50382022e+00  1.01310524e+00 -1.82013511e+00]
 [ 9.87469637e-01 -2.77546658e+00  1.79053529e-01 -3.58576457e+00]
 [ 9.45515789e-01  3.04802796e+00 -1.00888303e+00 -5.50510612e+00]
 [ 7.90024771e-01  2.44191693e+00 -2.07302498e+00 -6.39242204e+00]
 [ 5.69066649e-01  1.84642530e+00 -2.34572966e+00 -5.40743836e+00]
 [ 3.49371236e-01  1.37908468e+00 -2.06294093e+00 -3.96781371e+00]
 [ 1.69829833e-01  1.03840287e+00 -1.53564349e+00 -2.91262355e+00]
 [ 4.01232933e-02  7.85079690e-01 -1.06088952e+00 -2.21289853e+00]
 [-7.29728644e-02  6.18848504e-01 -1.20313366e+00 -1.13640286e+00]
 [-1.72321266e-01  5.19172609e-01 -7.83194563e-01 -8.83941291e-01]
 [-2.24906746e-01  4.31348767e-01 -2.67529409e-01 -8.97910536e-01]
 [-2.36960675e-01  3.49936300e-01  2.72103784e-02 -7.51936221e-01]
 [-2.21924717e-01  2.80672277e-01  2.74222491e-01 -6.51656969e-01]
 [-1.83716304e-01  2.18688680e-01  4.90629923e-01 -6.04391852e-01]
 [-1.27428162e-01  1.62107749e-01  6.35761822e-01 -5.42012017e-01]
 [-5.86786710e-02  1.10769464e-01  7.39811703e-01 -4.98131435e-01]
 [ 1.76273101e-02  6.43753262e-02  7.86839921e-01 -4.41802640e-01]
 [ 9.35391377e-02  2.79166126e-02  7.31817442e-01 -2.96843301e-01]
 [ 1.63085070e-01  4.93673379e-03  6.59372476e-01 -1.68758285e-01]
 [ 2.31159789e-01 -1.51507521e-02  7.02356850e-01 -2.38185971e-01]
 [ 3.01354748e-01 -4.05795013e-02  7.01839791e-01 -2.76978941e-01]
 [ 3.68186994e-01 -6.69274767e-02  6.35116562e-01 -2.56848431e-01]
 [ 4.21805817e-01 -8.34682588e-02  4.37466532e-01 -7.83887764e-02]
 [ 4.57804315e-01 -8.61149152e-02  2.82538272e-01  2.47236980e-02]
 [ 4.79423831e-01 -8.00401076e-02  1.49781309e-01  9.83379068e-02]]

        reward = -10*(2*ob[0]**2 + 3*ob[1]**2  + 1.2*ob[2]**2 + 1.2*ob[3]**2) - 0.6*a[0]**2


