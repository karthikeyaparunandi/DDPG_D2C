Using TensorFlow backend.
running build_ext
(2,)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
state_input (InputLayer)     (None, 1, 2)              0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 2)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 400)               1200      
_________________________________________________________________
activation_1 (Activation)    (None, 400)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 300)               120300    
_________________________________________________________________
activation_2 (Activation)    (None, 300)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 301       
_________________________________________________________________
activation_3 (Activation)    (None, 1)                 0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 1)                 0         
=================================================================
Total params: 121,801
Trainable params: 121,801
Non-trainable params: 0
_________________________________________________________________
None
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
observation_input (InputLayer)  (None, 1, 2)         0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 2)            0           observation_input[0][0]          
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 3)            0           action_input[0][0]               
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 400)          1600        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 400)          0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 300)          120300      activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 300)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            301         activation_5[0][0]               
==================================================================================================
Total params: 122,201
Trainable params: 122,201
Non-trainable params: 0
__________________________________________________________________________________________________
None
2019-02-09 08:06:22.256263: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Training for 300000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 60s 6ms/step - reward: -181.3383
333 episodes - episode_reward: -5440.583 [-72871.947, -4249.143] - loss: 36879.145 - mean_squared_error: 73758.290 - mean_q: -750.651

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 58s 6ms/step - reward: -143.8283
333 episodes - episode_reward: -4315.779 [-6147.026, -3183.692] - loss: 51020.633 - mean_squared_error: 102041.266 - mean_q: -1610.280

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 58s 6ms/step - reward: -140.3179
334 episodes - episode_reward: -4209.513 [-6174.733, -3056.220] - loss: 82625.211 - mean_squared_error: 165250.422 - mean_q: -2175.632

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 60s 6ms/step - reward: -144.5719
333 episodes - episode_reward: -4335.846 [-6462.689, -2307.572] - loss: 97755.477 - mean_squared_error: 195510.953 - mean_q: -2501.670

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 62s 6ms/step - reward: -76.2063
333 episodes - episode_reward: -2288.042 [-2817.312, -1973.094] - loss: 73519.031 - mean_squared_error: 147038.062 - mean_q: -2167.724

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 66s 7ms/step - reward: -69.5090
334 episodes - episode_reward: -2087.176 [-2583.318, -1909.885] - loss: 41959.152 - mean_squared_error: 83918.305 - mean_q: -1594.850

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 73s 7ms/step - reward: -66.2412
333 episodes - episode_reward: -1983.552 [-2318.345, -1906.342] - loss: 25357.111 - mean_squared_error: 50714.223 - mean_q: -1175.759

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 76s 8ms/step - reward: -64.7818
333 episodes - episode_reward: -1945.370 [-2465.842, -1891.522] - loss: 17741.998 - mean_squared_error: 35483.996 - mean_q: -925.387

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 79s 8ms/step - reward: -64.2824
334 episodes - episode_reward: -1930.308 [-2124.766, -1889.601] - loss: 14151.346 - mean_squared_error: 28302.691 - mean_q: -790.625

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 81s 8ms/step - reward: -64.8203
333 episodes - episode_reward: -1940.855 [-2287.326, -1889.681] - loss: 11703.856 - mean_squared_error: 23407.713 - mean_q: -703.825

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: -64.7520
333 episodes - episode_reward: -1944.501 [-2401.367, -1887.697] - loss: 10903.264 - mean_squared_error: 21806.527 - mean_q: -655.955

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: -64.4838
334 episodes - episode_reward: -1936.338 [-2174.893, -1886.999] - loss: 10088.885 - mean_squared_error: 20177.770 - mean_q: -620.119

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: -64.7350
333 episodes - episode_reward: -1938.517 [-2295.929, -1887.385] - loss: 9182.418 - mean_squared_error: 18364.836 - mean_q: -595.371

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: -64.3799
333 episodes - episode_reward: -1933.106 [-2380.985, -1891.440] - loss: 8754.635 - mean_squared_error: 17509.270 - mean_q: -566.717

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 95s 10ms/step - reward: -64.4383
334 episodes - episode_reward: -1934.973 [-2167.932, -1888.155] - loss: 7858.232 - mean_squared_error: 15716.464 - mean_q: -546.739

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 98s 10ms/step - reward: -64.6972
333 episodes - episode_reward: -1937.541 [-2257.698, -1889.274] - loss: 7544.038 - mean_squared_error: 15088.075 - mean_q: -530.407

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 109s 11ms/step - reward: -64.3305
333 episodes - episode_reward: -1931.366 [-2112.994, -1889.887] - loss: 6912.714 - mean_squared_error: 13825.429 - mean_q: -516.657

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 107s 11ms/step - reward: -64.1445
334 episodes - episode_reward: -1926.275 [-2112.765, -1888.364] - loss: 6718.072 - mean_squared_error: 13436.144 - mean_q: -503.487

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 110s 11ms/step - reward: -64.1811
333 episodes - episode_reward: -1921.969 [-2167.713, -1886.795] - loss: 6690.424 - mean_squared_error: 13380.848 - mean_q: -488.842

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 112s 11ms/step - reward: -64.0634
333 episodes - episode_reward: -1923.492 [-2222.250, -1888.349] - loss: 5633.136 - mean_squared_error: 11266.272 - mean_q: -476.927

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 115s 11ms/step - reward: -64.0282
334 episodes - episode_reward: -1922.720 [-2117.296, -1886.633] - loss: 5671.936 - mean_squared_error: 11343.871 - mean_q: -466.068

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 118s 12ms/step - reward: -64.2034
333 episodes - episode_reward: -1922.535 [-2164.577, -1885.914] - loss: 5624.627 - mean_squared_error: 11249.254 - mean_q: -456.220

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 120s 12ms/step - reward: -64.0325
333 episodes - episode_reward: -1922.699 [-2271.379, -1886.263] - loss: 4848.105 - mean_squared_error: 9696.211 - mean_q: -448.539

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 124s 12ms/step - reward: -64.3920
334 episodes - episode_reward: -1933.582 [-2136.358, -1886.948] - loss: 4872.463 - mean_squared_error: 9744.927 - mean_q: -441.880

Interval 25 (240000 steps performed)
 9950/10000 [============================>.] - ETA: 0s - reward: -64.1634^Cdone, took 2261.154 seconds
Creating window glfw
-1830.94786628251 [[ 3.14159265  0.        ]
 [-2.91606556  2.25527095]
 [-2.56303995  3.53025604]
 [-2.17706453  3.85975428]
 [-1.80398372  3.73080809]
 [-1.45733196  3.46651759]
 [-1.1727574   2.84574562]
 [-0.93690439  2.35853004]
 [-0.74575789  1.91146501]
 [-0.59263836  1.53119527]
 [-0.471784    1.20854367]
 [-0.37531097  0.96473026]
 [-0.29675796  0.7855301 ]
 [-0.23615237  0.60605595]
 [-0.19251632  0.4363605 ]
 [-0.16292091  0.29595403]
 [-0.14352011  0.194008  ]
 [-0.13088792  0.126322  ]
 [-0.12124444  0.09643471]
 [-0.11215146  0.09092989]
 [-0.10518404  0.06967415]
 [-0.09863554  0.065485  ]
 [-0.09360058  0.05034958]
 [-0.08888308  0.04717504]
 [-0.08524416  0.03638919]
 [-0.08184495  0.0339921 ]
 [-0.07885666  0.02988292]
 [-0.0757156   0.03141057]
 [-0.07270017  0.03015435]
 [-0.06967187  0.03028295]
 [-0.0666986   0.02973274]]

        reward = -10*(2*ob[0]**2 + ob[1]**2) - 3*a[0]**2

