_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
state_input (InputLayer)     (None, 1, 2)              0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 2)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 400)               1200      
_________________________________________________________________
activation_1 (Activation)    (None, 400)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 300)               120300    
_________________________________________________________________
activation_2 (Activation)    (None, 300)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 301       
_________________________________________________________________
activation_3 (Activation)    (None, 1)                 0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 1)                 0         
=================================================================
Total params: 121,801
Trainable params: 121,801
Non-trainable params: 0
_________________________________________________________________
None
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
observation_input (InputLayer)  (None, 1, 2)         0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 2)            0           observation_input[0][0]          
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 3)            0           action_input[0][0]               
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 400)          1600        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 400)          0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 300)          120300      activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 300)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            301         activation_5[0][0]               
==================================================================================================
Total params: 122,201
Trainable params: 122,201
Non-trainable params: 0
__________________________________________________________________________________________________
None
2019-02-08 15:42:35.182093: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Training for 300000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 62s 6ms/step - reward: -85.3990
333 episodes - episode_reward: -2562.068 [-38775.121, -1860.000] - loss: 19503.101 - mean_squared_error: 39006.203 - mean_q: -386.854

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 61s 6ms/step - reward: -68.4412
333 episodes - episode_reward: -2053.494 [-2541.289, -1345.811] - loss: 13550.223 - mean_squared_error: 27100.445 - mean_q: -851.234

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 63s 6ms/step - reward: -46.4192
334 episodes - episode_reward: -1394.048 [-2647.102, -1218.103] - loss: 17221.541 - mean_squared_error: 34443.082 - mean_q: -974.858

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 66s 7ms/step - reward: -42.5188
333 episodes - episode_reward: -1273.544 [-1497.290, -1205.625] - loss: 9488.599 - mean_squared_error: 18977.197 - mean_q: -718.640

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 67s 7ms/step - reward: -41.1951
333 episodes - episode_reward: -1236.800 [-1411.185, -1203.488] - loss: 5658.031 - mean_squared_error: 11316.062 - mean_q: -537.921

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 70s 7ms/step - reward: -68.5157
334 episodes - episode_reward: -2054.948 [-29710.739, -1211.644] - loss: 4564.273 - mean_squared_error: 9128.546 - mean_q: -411.752

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 72s 7ms/step - reward: -41.9840
333 episodes - episode_reward: -1257.729 [-1513.145, -1203.265] - loss: 3380.440 - mean_squared_error: 6760.880 - mean_q: -417.378

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 75s 8ms/step - reward: -40.6750
333 episodes - episode_reward: -1220.951 [-1381.349, -1199.377] - loss: 3374.318 - mean_squared_error: 6748.636 - mean_q: -389.803

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 78s 8ms/step - reward: -40.5330
334 episodes - episode_reward: -1217.127 [-1319.538, -1200.882] - loss: 2753.854 - mean_squared_error: 5507.709 - mean_q: -368.625

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 81s 8ms/step - reward: -40.6402
333 episodes - episode_reward: -1217.237 [-1268.865, -1199.311] - loss: 2545.577 - mean_squared_error: 5091.153 - mean_q: -361.353

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: -40.5374
333 episodes - episode_reward: -1216.937 [-1267.957, -1200.355] - loss: 2520.643 - mean_squared_error: 5041.287 - mean_q: -357.307

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: -40.3609
334 episodes - episode_reward: -1211.993 [-1266.878, -1198.635] - loss: 2105.755 - mean_squared_error: 4211.510 - mean_q: -352.020

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: -40.4739
333 episodes - episode_reward: -1212.278 [-1264.105, -1199.051] - loss: 2019.912 - mean_squared_error: 4039.823 - mean_q: -342.667

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 92s 9ms/step - reward: -40.3817
333 episodes - episode_reward: -1212.220 [-1308.702, -1198.016] - loss: 1941.977 - mean_squared_error: 3883.954 - mean_q: -332.851

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 95s 9ms/step - reward: -40.3714
334 episodes - episode_reward: -1212.313 [-1267.242, -1198.717] - loss: 1710.295 - mean_squared_error: 3420.590 - mean_q: -326.311

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 98s 10ms/step - reward: -40.4367
333 episodes - episode_reward: -1211.203 [-1279.233, -1197.633] - loss: 1606.722 - mean_squared_error: 3213.445 - mean_q: -318.885

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 101s 10ms/step - reward: -40.2762
333 episodes - episode_reward: -1209.015 [-1272.425, -1198.248] - loss: 1574.535 - mean_squared_error: 3149.070 - mean_q: -314.423

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 106s 11ms/step - reward: -40.2143
334 episodes - episode_reward: -1207.603 [-1313.447, -1197.686] - loss: 1489.563 - mean_squared_error: 2979.127 - mean_q: -308.099

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 110s 11ms/step - reward: -40.3711
333 episodes - episode_reward: -1209.231 [-1285.416, -1197.975] - loss: 1571.825 - mean_squared_error: 3143.650 - mean_q: -304.739

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 112s 11ms/step - reward: -40.3309
333 episodes - episode_reward: -1210.655 [-1307.311, -1198.580] - loss: 1244.950 - mean_squared_error: 2489.900 - mean_q: -299.084

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 115s 11ms/step - reward: -40.2712
334 episodes - episode_reward: -1209.312 [-1260.559, -1197.985] - loss: 1359.987 - mean_squared_error: 2719.974 - mean_q: -296.685

Interval 22 (210000 steps performed)
  211/10000 [..............................] - ETA: 1:57 - reward: -40.9639^Cdone, took 1790.462 seconds
Creating window glfw
-1201.3902123374248 [[ 3.14159265  0.        ]
 [-2.93530117  2.06291484]
 [-2.64856961  2.86731558]
 [-2.35700229  2.91567324]
 [-2.08527809  2.71724202]
 [-1.8290443   2.56233789]
 [-1.5966468   2.32397501]
 [-1.35713331  2.39513488]
 [-1.15948723  1.97646073]
 [-0.99020055  1.69286686]
 [-0.84505696  1.45143591]
 [-0.7221819   1.22875054]
 [-0.61626118  1.0592072 ]
 [-0.5231976   0.93063587]
 [-0.44021251  0.82985086]
 [-0.36541084  0.74801673]
 [-0.29762223  0.67788605]
 [-0.24866198  0.48960256]
 [-0.21896075  0.29701224]
 [-0.19990926  0.19051489]
 [-0.18595508  0.13954184]
 [-0.17480053  0.11154545]
 [-0.16544284  0.09357699]
 [-0.15740082  0.08042012]
 [-0.15040948  0.06991339]
 [-0.14429842  0.06111069]
 [-0.1389428   0.05355613]
 [-0.1342431   0.04699704]
 [-0.13011611  0.04126985]
 [-0.12649054  0.03625579]
 [-0.12330456  0.0318598 ]]

reward -         reward = -10*(ob[0]**2 + ob[1]**2) - a[0]**2

no terminal reward

